[["index.html", "Fitting robust non-Gaussian models in Stan Chapter 1 About 1.1 What and Why 1.2 Setup 1.3 This Bookdown 1.4 Citation", " Fitting robust non-Gaussian models in Stan Rafael Cabral 2022-09-07 Chapter 1 About 1.1 What and Why Traditionally the driving or excitation noise of spatial and temporal models is Gaussian. Take for instance an AR1 (autoregressive of order 1) process, where the increments \\(x_{i+1}-\\rho x_i, |\\rho|&lt;1\\) are assumed to follow a Gaussian distribution. However, real-world data may not be Gaussian in nature, and it is well known that outliers can adversely affect the inferences and predictions made from a Gaussian model. We explicate in this Bookdown how to implement a generic class of non-Gaussian models in Stan that have the same mean and covariance structure as Gaussian models, although having heavier-tails and skewed marginals, and more flexible sample path behaviors. This generic class contains many models used in applications as special cases, such as AR and random walk (RW) processes for time series, spatial Matérn models, and SAR and CAR models for areal data. 1.2 Setup If \\(\\mathbf{x} \\sim \\text{N}(\\mathbf{0}, (\\mathbf{D}^T\\text{diag}(\\mathbf{h})\\mathbf{D})^{-1})\\), then it can be declared in Stan as: x ~ multi_normal_prec(rep_vector(0,N), D&#39;*diag_matrix(1/h)*D) The non-Gaussian model declaration is: x ~ nig_model(D, etas, zetas, h, 1) where etas, zetas are parameters that regulate the kurtosis and skewness of the model, and the last argument is an integer with value 1 if the log-determinant of \\(\\mathbf{D}\\) should be computed (if \\(\\mathbf{D}\\) depends on parameters), or 0 otherwise. The nig_model and other Stan functions can be found in nigstan\\files\\functions.stan on github.com/rafaelcabral96. 1.3 This Bookdown This Bookdown is organized as follows: Section 2: Contains a theoretical introduction to the non-Gaussian models we are studying and several details for the analytically inclined that you may skip. Section 3: Shows how to use the different Stan functions to fit non-Gaussian models. Section 4: Compares the performance of the 3 implementations we propose. Section 5: Time-series application Section 6: Geostatistics application Section 7: Areal data application (SAR and CAR models) 1.4 Citation If you use the code related to this project please cite Cabral, Bolin, and Rue (2022). References "],["theoretical-background.html", "Chapter 2 Theoretical background 2.1 NIG distribution 2.2 New parameterization 2.3 Framework for extending Gaussian models 2.4 Penalized complexity priors for \\(\\eta^\\star\\) and \\(\\zeta^\\star\\) 2.5 Useful properties of the vector \\(\\mathbf{x}\\)", " Chapter 2 Theoretical background 2.1 NIG distribution Instead of considering a Gaussian driving noise for the stochastic processes, we will use the NIG distribution, which is a more flexible distribution, allowing for exponential tails and asymmetry. The common parameterization of the NIG distribution has 4 parameters (\\(\\mu\\), \\(\\delta\\), \\(\\alpha\\), \\(\\beta\\)) and the following pdf: \\[ \\pi(x) = \\frac{\\alpha \\delta K_{1}\\left(\\alpha \\sqrt{\\delta^{2}+(x-\\mu)^{2}}\\right)}{\\pi \\sqrt{\\delta^{2}+(x-\\mu)^{2}}} e^{\\delta \\gamma+\\beta(x-\\mu)}, \\] where \\(\\gamma= \\sqrt{\\alpha^2-\\beta^2}\\), and \\(K_\\nu(x)\\) is the modified Besse function of the second kind. A NIG random variable \\(\\Lambda\\) can be represented as a Normal variance-mean mixture: \\[ \\Lambda|V \\sim N(\\mu+ \\beta V , V) \\\\ V \\sim IG(\\delta,\\gamma), \\] where IG stands for the Inverse-Gaussian distribution. This representation is useful for sampling and is also the basis for Bayesian and maximum likelihood estimation (see Bolin (2014) and Walder and Hanks (2020)) since the normal variance-mean mixture shares many of the convenient properties of the Gaussian distribution. 2.2 New parameterization 2.2.1 Mean-scale invariant parameterization ($, $) In the previous parameterization, the location (\\(\\mu\\)) and scale (\\(\\delta\\)) parameters do not correspond to the mean and standard deviation of the distribution, which also depends on \\(\\alpha\\) and \\(\\beta\\). In Cabral, Bolin, and Rue (2022) we propose a “mean-scale” invariant parameterization where the mean and the standard deviation are fixed. This not only eases interpretation but also allows assigning the priors for the mean and scale of the Gaussian model in non-Gaussian models, which is very convenient when implementing these non-Gaussian models in practice. The mixture representation belonging to this parameterization is: \\[\\begin{equation} \\Lambda|V \\sim N\\left(\\zeta \\tilde{\\sigma}(V-1), \\ \\ \\tilde{\\sigma}^2V\\right), \\ \\ \\ \\ \\tilde{\\sigma} = \\frac{1}{\\sqrt{1+\\eta \\zeta^2}}, \\\\ V \\sim IG(1,\\eta^{-1}) \\tag{2.1} \\end{equation}\\] The mean and variance are always \\(0\\) and \\(1\\), respectively. The parameter \\(\\eta\\) is related to the degree of non-Gaussianity since \\(\\Lambda\\) converges to a Gaussian random variable when \\(\\eta \\to 0\\), and to a Cauchy random variable when \\(\\eta \\to \\infty\\). The parameter \\(\\zeta\\) is related to the asymmetry of the random variable since \\(\\Lambda\\) is a symmetric random variable when \\(\\zeta=0\\), and when \\(\\zeta&gt;0\\) it is skewed to the right. Also, the larger \\(\\eta\\), the larger the asymmetry induced by the same value of \\(\\zeta\\) (see following plots). knitr::include_graphics(&quot;../files/images/NIGplot.png&quot;) Figure 2.1: PDF of NIG distribution in log scale 2.2.2 Standardized and orthogonal parameterization (\\(\\eta^\\star\\), \\(\\zeta^\\star\\)) There is still some confounding between \\(\\eta\\) and \\(\\zeta\\) in the standardized parameterization since the excess kurtosis increases with \\(\\zeta\\). We find the kurtosis to be hard to interpret (it is not clear what an increase in kurtosis of 1 means in practice) and so we prefer to associate \\(\\eta\\) with the likelihood of large events. For the NIG distribution this probability can be approximated by for large \\(x\\) . The dependency of this probability with the skewness parameter \\(\\zeta\\) comes through the rate \\(\\xi\\): \\[\\xi = 1+\\zeta^2\\eta - |\\zeta|\\sqrt{\\eta(1+\\zeta^2\\eta)},\\] which is equal to 1 in the symmetric case (\\(\\zeta=0\\)). Cabral, Bolin, and Rue (2022) required the probability \\(P(|\\Lambda|&gt;x)\\) to be invariant with the skewness parameter, at least for large \\(x\\). This can be achieved by the parameter transformations \\(\\eta^\\star = \\eta\\xi_{NIG}^{-2}\\) and \\(\\zeta^\\star = \\zeta\\sqrt{\\eta}\\). This parameterization has an orthogonal interpretation since the likelihood of large events and the excess Kurtosis now only depend on \\(\\eta^{\\star}\\). 2.3 Framework for extending Gaussian models 2.3.1 Illustration with the RW1 process Here we show how a random walk of order 1 (RW1) process with irregularly spaced locations can be extended to non-Gaussianity. It is quite a simple model, but it serves as a good illustration since the same procedure applies to more complex models. Consider a set of \\(n\\) locations \\(s_1,\\dotsc, s_n\\), where the distance between locations is \\(h_i=s_{i+1}-s_i\\). When the driving noise is Gaussian, we assume \\(x_{i+1}-x_i \\overset{d}{=} \\sigma Z_i\\), where \\(Z_i \\sim N(0,h_i)\\). Notice that the RW1 model can be seen as an approximation of a Wiener process. In our non-Gaussian setting, we replace the standardized Gaussian noise \\(Z_i\\) with standardized non-Gaussian noise \\(\\Lambda_i\\), so we assume that \\(x_{i+1}-x_i \\overset{d}{=} \\sigma\\Lambda_i\\), where \\(\\Lambda_i\\) follows: \\[\\begin{equation} \\Lambda_i|V_i \\sim N\\left(\\zeta \\tilde{\\sigma}(V_i-h_i), \\ \\ \\tilde{\\sigma}^2V\\right), \\ \\ \\ \\ \\tilde{\\sigma} = \\frac{\\sigma}{\\sqrt{1+\\eta \\zeta^2}} \\\\ V_{i} \\overset{ind.}{\\sim} IG(h_i,\\eta^{-1} h_i^2) \\\\ \\tag{2.2} \\end{equation}\\] where \\(E[\\Lambda_i]=0\\) and \\(Var[\\Lambda_i]=h_i\\). For the analytically inclined, this RW1 model is an approximation of a Lévy process \\(L(t)\\), where an increment of size 1 follows the NIG distribution. Speaking only of essential terms, a Lévy process is a process with independent and stationary increments, and all finite-dimensional distributions are known if we know the distribution of an increment (more about Lévy processes in Ken-Iti (1999) and Barndorff-Nielsen, Mikosch, and Resnick (2012)). If the increments are Gaussian, then \\(L(t)\\) is a Gaussian process, while in general, it is non-Gaussian. Next, we show a sample path of RW1 with \\(\\sigma=1\\) near the Gaussian case by setting \\(\\eta=10^{-6}\\) (top) and a non-Gaussian case by setting \\(\\eta=10\\) (bottom). We generated these simulated paths, by first sampling the mixing variables \\(V_i\\), then sampling the noise \\(\\Lambda_i|V_i\\), and then the RW1 sample paths are just a cumulative sum of the noises. Notice that, in the first panel, the mixing variables \\(V_i\\) are very close to 1 while on the bottom panel there is more discrepancy. When \\(V_i\\) takes a large value, the noise \\(\\Lambda\\) can also be large and then we observe a large jump in the RW1 process driven by NIG noise. knitr::include_graphics(&quot;../files/images/GaussRW1.png&quot;) Figure 2.2: Sample of mixing vector \\(\\mathbf{V}\\), noise vector \\(\\mathbf{\\Lambda}\\) and RW1 vector \\(\\mathbf{x}\\) knitr::include_graphics(&quot;../files/images/NIGRW1.png&quot;) Figure 2.3: Sample of mixing vector \\(\\mathbf{V}\\), noise vector \\(\\mathbf{\\Lambda}\\) and RW1 vector \\(\\mathbf{x}\\) 2.3.2 Models defined via \\(\\mathbf{D}\\mathbf{x} = \\mathbf{Z}\\) The system of equations seen before \\(x_{i+1}-x_i \\overset{d}{=} Z_i\\) defines the linear system \\(\\mathbf{D}_{RW1}\\mathbf{x}=\\sigma\\mathbf{Z}\\), where \\(\\mathbf{Z}\\) is a vector of independent Gaussian noise, and: \\[ \\mathbf{D}_{RW1} = \\begin{pmatrix} -1 &amp; 1 &amp; &amp; &amp; \\\\ &amp; -1 &amp; 1 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; -1 &amp; 1 \\end{pmatrix}. \\] For a RW2 model it is assumed \\(x_{i+2}-2x_{i+1}+x_i = \\sigma Z_i\\), and for an autoregressive process of order 1 (AR1) the assumptions are that \\(\\sqrt{1-\\rho^2}x_1= \\sigma Z_1\\), and \\(x_{i+1}-\\rho x_i = \\sigma Z_i\\) for \\(i&gt;1\\), where \\(\\rho\\) is the autocorrelation parameter. Thus, the matrices \\(\\mathbf{D}_{RW2}\\) and \\(\\mathbf{D}_{AR1}\\) are: \\[ \\mathbf{D}_{RW2} = \\begin{pmatrix} 1 &amp; -2 &amp; 1 &amp; &amp; &amp; \\\\ &amp; 1 &amp; -2 &amp; 1 &amp; &amp; \\\\ \\\\ &amp; &amp; &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\\\ \\\\ &amp; &amp; &amp; &amp; &amp; 1 &amp; -2 &amp; 1 \\end{pmatrix}, \\ \\ \\mathbf{D}_{AR1} = \\begin{pmatrix} \\sqrt{1-\\rho^2} &amp; &amp; &amp; &amp; &amp; \\\\ -\\rho &amp; 1 &amp; &amp; &amp; &amp; \\\\ &amp; -\\rho &amp; 1 &amp; &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; -\\rho &amp; 1 \\end{pmatrix} \\] Likewise, higher-order AR processes can be constructed. The rows of the matrix \\(\\mathbf{D}\\) contain the “increments” that we assume to follow Gaussian noise. Other models fit in this framework, including intrinsic conditional autoregressive models and simultaneous autoregressive models which we will see later in the applications. The system \\(\\mathbf{D}\\mathbf{x} = \\sigma\\mathbf{Z}\\) also appears when computing discrete-space approximation of stochastic processes defined via SPDEs: \\(\\mathcal{D}X(t) = W&#39;(t)\\), where \\(\\mathcal{D}\\) is a linear differential operator and \\(W&#39;(t)\\) is a Gaussian noise process. For instance, stationary Mat'ern random fields and Ornstein–Uhlenbeck processes arise as stationary solutions to SPDEs. 2.3.3 Generic framework We have seen before that the linear system \\[ \\mathbf{D}\\mathbf{x}^G\\overset{d}{=} \\mathbf{Z}, \\] is a natural representation for many models, where the matrix \\(\\mathbf{D}\\) defines the model and \\(\\mathbf{Z}\\) is a vector of independent Gaussian noise: \\(Z_i \\overset{i.i.d}{\\sim} N(0,h_i)\\), where \\(h_i\\) is the distance between locations, for instance. The precision matrix of \\(\\mathbf{x}\\) is then \\(\\mathbf{Q}\\propto \\mathbf{D}^T\\text{diag}(\\mathbf{h})^{-1}\\mathbf{D}\\), which for the examples we will study is sparse. The non-Gaussian extension consists in assuming: \\[ \\mathbf{D}\\mathbf{x}\\overset{d}{=} \\mathbf{\\Lambda}, \\] where \\(\\boldsymbol{\\Lambda}\\) is a vector of independent NIG noise as in eq. (2.2). A location and scale parameter can be added by the usual transformation \\(\\mu + \\sigma\\mathbf{x}\\). Consider the vector \\(\\mathbf{V}=[V_1,V_2, \\dotsc, V_n]^T\\). If \\(\\mathbf{D}\\) is a full rank matrix, then \\(\\mathbf{x}\\) has the representation: \\[\\begin{equation} \\mathbf{x}|\\boldsymbol{V} \\sim N\\left(\\tilde{\\sigma}\\zeta \\mathbf{D}^{-1}(\\boldsymbol{V}-\\boldsymbol{h}), \\ \\ \\tilde{\\sigma}^2 \\mathbf{D}^{-1}\\text{diag}(\\boldsymbol{V})\\mathbf{D}^{-T}\\right)\\\\ V_{i} \\overset{ind.}{\\sim} IG(h_i,\\eta^{-1} h_i^2) \\\\ \\tag{2.3} \\end{equation}\\] where \\(\\mathbf{x}|\\boldsymbol{V}\\) is still a multivariate Gaussian distribution, but with precision matrix \\(Q=\\tilde{\\sigma}^{-2}\\mathbf{D}^T \\text{diag} ({\\mathbf{V}})^{-1}\\mathbf{D}\\) with the same degree of sparsity as in the Gaussian case. Here it is important to mention that the precision or covariance matrices do not uniquely define the models, and these are only uniquely defined by the matrix \\(\\mathbf{D}\\). For instance, if we decompose the covariance matrix in two ways, \\(\\Sigma = L_1 L_1^T\\) and \\(\\Sigma = L_2 L_2^T\\), then in the Gaussian case, the vectors \\(\\mathbf{x_1}=\\mathbf{L_1}\\mathbf{Z}\\) and \\(\\mathbf{x_2}=\\mathbf{L_2}\\mathbf{Z}\\) are equal in distribution. However, when the noise is non-Gaussian this is not the case. An example is shown in Fig. 2 of Asar et al. (2020), where stochastic processes with associated operators \\(\\mathcal{D}_1=\\kappa + \\partial_t\\), \\(\\mathcal{D}_2 =\\kappa^2- \\partial_t^2\\) have the same exponential covariance function, but when using a NIG noise process the sample paths behave differently. This limits the transformations we can do for computational efficiency because we are restricted to the model representation \\(\\mathbf{D}\\mathbf{x}=\\mathbf{\\Lambda}\\). 2.3.4 Sample paths We look at different models that can be expressed through: \\[ \\mathcal{D}X(t) =L&#39;(t), \\] where \\(\\mathcal{D}\\) is a linear differential operator and \\(L&#39;(t)\\) is a NIG noise process. The differential operator for different models is shown next, where OU stands for the Ornstein–Uhlenbeck process, which can be seen as the continuous version of an autoregressive process of order 1. In the third column, we consider the discrete case equivalents of these models, where the ‘increments’ are assumed to follow independent NIG noise. Model \\(\\mathcal{D}\\) Increments RW1 \\(\\partial_t\\) \\(x_{i+1}-x_{i}\\) OU \\(\\kappa + \\partial_t\\) \\(x_{i+1} + (\\kappa-1)x_{i}\\) RW2 \\(\\partial_t^2\\) \\(x_{i} - 2x_{i+1} + x_{i+2}\\) Matérn \\(\\alpha=2\\) \\(\\kappa^2- \\partial_t^2\\) \\((1+\\kappa^2)x_{i} - 2x_{i+1} + x_{i+2}\\) The first column of the next plot shows a simulation of Gaussian noise and the sample paths of several models generated from it. In the second column, we repeat the same, but now with non-Gaussian NIG noise. We can see that most NIG noise events are near 0, and a few of them take large values (larger than the ones you see in the Gaussian noise process) since the NIG distribution has heavier tails and is more peaked around 0. Whenever the noise takes a large value (for instance, near location 0.25), the RW1 and OU processes will exhibit a distinct jump, and the RW2 and Matérn processes will exhibit a kink (discontinuity in the first derivative). knitr::include_graphics(&quot;../files/images/sim1.png&quot;) Figure 2.4: Noise and sample paths for several models Next, we show sample paths of a Matérn model in 2D driven with NIG driving noise, as described in Bolin (2014). Notice that in Fig. 2.5 as \\(\\eta\\) increases we observe more “hotspots” and “coldspots”, that is, regions where the field \\(X\\) takes values that would be considered extreme in a Gaussian model. In the symmetric case (\\(\\zeta = 0\\)) the number of hotspots and coldspots is on average the same, while if \\(\\zeta &gt; 0\\) there should be on average more hotspots than coldspots. knitr::include_graphics(&quot;../files/images/sim3.png&quot;) Figure 2.5: Sample paths of a Matérn model in 2D with \\(\\alpha=2\\) 2.4 Penalized complexity priors for \\(\\eta^\\star\\) and \\(\\zeta^\\star\\) The non-Gaussian extension does not affect the mean and covariance structure of the model so we can reuse the same priors for the scale \\(\\sigma\\) and spatial range parameter \\(\\kappa\\) from the Gaussian model. Prior distributions for \\(\\eta^\\star\\) and \\(\\zeta^\\star\\) are constructed based on the penalized complexity (PC) priors approach (Simpson et al. (2017)) in Cabral, Bolin, and Rue (2022). PC priors tend to avoid overfitting by default because the mode of the prior distribution is located at the base model. A useful result shown in Cabral, Bolin, and Rue (2022) is that the PC priors for \\(\\eta^\\star\\) and \\(\\zeta^\\star\\) do not depend on the matrix \\(\\mathbf{D}\\) or the scale \\(\\sigma\\), so the prior distributions will be the same regardless of the model. The PC prior for \\(\\eta^\\star\\) follows an exponential distribution and the PC prior for \\(\\zeta^\\star\\) follow a Laplace distribution, each with a given rate parameter. This rate parameter can be found by relating \\(\\eta^\\star\\) and \\(\\zeta^\\star\\) with some interpretable properties of the model. For \\(\\eta^\\star\\) we can study how many more large events the NIG noise has compared with the Gaussian noise: \\(Q(\\eta^\\star) = P(|\\Lambda|&gt;3)/P(|Z|&gt;3)\\), where \\(\\Lambda\\) and \\(Z\\) are standardized NIG and Gaussian random variable. Then, \\(\\eta^\\star\\) is found from the probability statement \\(P(Q(\\eta^\\star)&gt;2)=\\alpha\\), where we set a low probability \\(\\alpha\\) that the NIG noise has twice as much large events as the Gaussian noise. A more detailed discussion on the PC priors for non-Gaussian models can be found in Cabral, Bolin, and Rue (2022) 2.5 Useful properties of the vector \\(\\mathbf{x}\\) In some examples we consider latent (non-Gaussian) models where the observations \\(\\mathbf{y}\\) depend on \\(\\mathbf{x}\\) through \\(\\mathbf{y}|\\mathbf{x} \\sim N(\\mathbf{B}\\boldsymbol{\\beta} + \\sigma_x\\mathbf{x},\\sigma_y)\\), where \\(\\mathbf{B}\\) is a design matrix, \\(\\boldsymbol{\\beta}\\) are regression coefficients, and \\(\\mathbf{x}\\) is the non-Gaussian random vector usually added to model structured temporal or spatial effects. Bayesian and maximum likelihood estimation of these models typically leverages the hierarchical representation in eq. (2.3). The precision matrix of \\(\\mathbf{x}|\\mathbf{V}\\) is sparse, but to compute the mean of \\(\\mathbf{x}|\\mathbf{V}\\) we need to invert the matrix \\(\\mathbf{D}\\), which can be expensive, especially if \\(\\mathbf{D}\\) depends on a parameter, since then this inversion needs to be done at every iteration. Furthermore, if we implement this representation directly on Stan, besides estimating, \\(\\sigma_x\\), \\(\\eta^\\star\\), and \\(\\zeta^\\star\\), we need an extra \\(2N\\) parameters, since both \\(\\mathbf{x}\\) and \\(\\mathbf{V}\\) need to be estimated, where \\(N\\) is the dimension of \\(\\mathbf{x}\\), 2.5.1 Joint PDF of \\(\\mathbf{x}\\) By integrating out the auxiliary variables \\(V_i\\) in eq. (2.3) we can basically cut the dimension of the parameter space being explored in Stan by almost half, which should lead to a significant speedup. This at first may seem complicated to do, but if we realize that \\(\\mathbf{x} = \\mathbf{D}^{-1}\\mathbf{\\Lambda}\\) if \\(\\mathbf{D}\\) is non-singular, then the multivariate transformation method wields: \\[\\begin{equation} \\pi(\\mathbf{x})= |\\mathbf{D}|\\prod_{i=1}^n\\pi_{\\Lambda_i}([\\mathbf{D}\\mathbf{x}]_i), \\tag{2.4} \\end{equation}\\] where \\(\\pi_{\\Lambda_i}\\) is the PDF of a NIG distribution with parameters \\(\\sigma\\), \\(\\eta^\\star\\) and \\(\\zeta^\\star\\), and \\(h_i\\) as in eq. (2.2). We can then do the estimation in Stan based on the previous joint density, where there is no need for the auxiliary vector \\(\\mathbf{V}\\) and no need of inverting the matrix \\(\\mathbf{D}\\). 2.5.2 Mixing distribution vector \\(\\mathbf{V}\\) The prior assumption on the mixing variables is \\(V_{i}|\\eta \\overset{ind.}{\\sim} IG(h_i,\\eta^{-1} h_i^2)\\). It can be shown that \\(V_{i} \\to h_i\\), in the Gaussian limit (when \\(\\eta \\to 0\\)). Therefore in the Gaussian limit \\(\\mathbf{V}\\oslash\\mathbf{h}=\\mathbf{1}\\), where \\(\\oslash\\) performs the element-wise division. If \\(V_i/h_i\\) has a high posterior mean, then the noise element \\(\\Lambda_i\\) is significantly non-Gaussian, and more flexibility is needed around time point or location \\(i\\) that is not present in the Gaussian model (see Fig. 2.3). Thus, the posterior summaries of \\(E[\\mathbf{V}\\oslash\\mathbf{h}]\\) can be used as a diagnostic tool to inspect if and where departures from Gaussianity occur. It is easier to interpret than the posterior summaries of \\(\\eta\\), as the “non-Gaussianity” of \\(\\mathbf{x}\\) depends not only on \\(\\eta\\), but also on \\(\\mathbf{D}\\) (Cabral, Bolin, and Rue (2022)). After the estimation is completed using the joint PDF of eq. (2.4) we can generate samples of \\(\\mathbf{V}|\\mathbf{y}\\) post-hoc using: \\[\\begin{equation} V_i|\\mathbf{x},\\tilde{\\sigma},\\eta,\\zeta,h_i \\sim GIG(-1,\\eta^{-1} + \\zeta^2,\\eta^{-1}h_i^2 + ([\\mathbf{D}\\mathbf{x}]_i/\\tilde{\\sigma} + \\zeta h_i)^2), \\tag{2.5} \\end{equation}\\] where \\(GIG(p, a,b)\\) stands for the generalized inverse Gaussian distribution with pdf: \\[ \\pi(x | p, a, b)=\\frac{(a / b)^{p / 2}}{2 K_{p}(\\sqrt{a b})} x^{p-1} \\exp \\left(-\\frac{a}{2} x-\\frac{b}{2} x^{-1}\\right),\\ \\ x&gt;0. \\] References "],["implementing-latent-models-driven-by-nig-noise-in-stan.html", "Chapter 3 Implementing latent models driven by NIG noise in Stan 3.1 Framework 3.2 Implementation 3.3 Additional functions 3.4 Notes", " Chapter 3 Implementing latent models driven by NIG noise in Stan Here we review the framework for extending Gaussian models to models driven with NIG noise and show how to declare these models in Stan using the suite of functions that we developed. These functions can be found in files\\functions.stan in the Github folder. 3.1 Framework Latent Gaussian models are a class of hierarchical models where the latent variable is Gaussian. It includes a large portion of models used in applications such as regression models, dynamic models, and spatial and temporal models (Rue, Martino, and Chopin (2009)). Their general form is: \\[ \\mathbf{y}|\\mathbf{x} \\sim \\pi(\\mathbf{y}|\\mathbf{x},\\mathbf{\\theta}_y) \\\\ \\mathbf{D}(\\mathbf{\\theta}_\\mathbf{x}) \\mathbf{x} = \\mathbf{Z}\\\\ \\mathbf{\\theta}_\\mathbf{x} \\sim \\pi(\\mathbf{\\theta}_\\mathbf{x}) \\] where the observations \\(y_i\\) are usually independent conditionally on the latent vector \\(\\mathbf{x}\\). In the applications that we will study, \\(\\mathbf{\\theta}_y\\) contains the regression coefficients among other parameters, and the vector \\(\\mathbf{\\theta}_\\mathbf{x}\\) usually includes a scale parameter \\(\\sigma_x\\) and a range parameter \\(\\kappa\\). The vector \\(\\mathbf{Z}\\) is comprised of independent Gaussian noise where \\(Z_i\\sim N(0,h_i)\\) and the latent vector \\(\\mathbf{x}\\) follows a Gaussian distribution with mean \\(\\mathbf{0}\\) and precision matrix \\(\\mathbf{D}^{T}\\text{diag}(\\mathbf{h})^{-1}\\mathbf{D}\\). The extension to non-Gaussianity consists in replacing the Gaussian noise \\(\\mathbf{Z}\\) with non-Gaussian noise \\(\\mathbf{\\Lambda}\\), which depends on two flexibility parameters. The parameter \\(\\eta^\\star\\) controls the heaviness of the tails, while \\(\\zeta^\\star\\) controls the asymmetry of the noise. \\[ \\mathbf{y}|\\mathbf{x} \\sim \\pi(\\mathbf{y}|\\mathbf{x},\\mathbf{\\theta}_y) \\\\ \\mathbf{D}(\\mathbf{\\theta}_\\mathbf{x}) \\mathbf{x} = \\mathbf{\\Lambda}(\\eta^\\star,\\zeta^\\star)\\\\ \\mathbf{\\theta}_\\mathbf{x} \\sim \\pi(\\mathbf{\\theta}_\\mathbf{x}) \\\\ \\eta^\\star \\sim \\text{Exp}(\\theta_{\\eta^\\star})\\\\ \\zeta^\\star \\sim \\text{Laplace}(\\theta_{\\zeta^\\star}) \\] 3.2 Implementation We consider in this example Gaussian observations given by \\(\\mathbf{y}|\\mathbf{x} \\sim \\text{Normal}(\\mathbf{B}\\boldsymbol{\\beta} + \\sigma_x\\mathbf{x}, \\sigma_\\epsilon^2\\mathbf{I})\\), where \\(\\mathbf{B}\\) is a design matrix and \\(\\boldsymbol{\\beta}\\) is a set of regression coefficients. The declaration of this model is: model{ //observation layer--------------------------- y ~ normal(B*beta + sigmax*x, sigmae); //latent field layer-------------------------- x ~ multi_normal_prec(rep_vector(0,N), D&#39;*diag_matrix(1/h)*D) //prior layer--------------------------------- ... } For the non-Gaussian latent model, we simply change the declaration of \\(\\mathbf{x}\\) as follows in the next code chunk and add the log-likelihoods of the priors for \\(\\eta^\\star\\) and \\(\\zeta^\\star\\). model{ //observation layer--------------------------- y ~ normal(B*beta + sigma*x, sigmae); //latent field layer-------------------------- x ~ nig_model(D, etas, zetas, h, 1) //prior layer--------------------------------- ... etas ~ exp(theta_eta) zetas ~ double_exponential(0,1.0/theta_zeta) } When declaring x ~ nig_model(...) the function nig_model_lpdf is called which has the following signature: real modelNIG_lpdf(vector x, matrix D, real etas, real zetas, vector h, int compute_det) x - vector \\(\\mathbf{x}\\) D - matrix \\(\\mathbf{D}\\) which defines the model etas - First flexibility parameter zetas - Second flexibility parameter h - Distance between locations, or area of basis functions. compute_det - Compute log determinant of \\(\\mathbf{D}\\) (1) or not (0) Returns - Log-likelihood of the random vector \\(\\mathbf{x}\\) where the driving noise uses the standardized and orthogonal parameterization. The function nig_model_lpdf computes the log of the joint density of x: \\[\\begin{equation} \\log \\pi(\\mathbf{x|\\eta^\\star,\\zeta^\\star})= \\log|\\mathbf{D}| + \\sum_{i=1}^n\\log\\pi_{\\Lambda_i(\\eta^\\star,\\zeta^\\star,h_i)}([\\mathbf{D}\\mathbf{x}]_i), \\tag{3.1} \\end{equation}\\] which is given by the log determinant of \\(\\mathbf{D}\\) plus the sum of NIG log-densities.nig_model also allows for within-chain parallelization through the reduce_sum function in Stan, which leverages on the fact that each term in the sum can be evaluated separately. To use this feature set model$sample(..., threads_per_chain = k), where k is the number of threads per chain and model is the CmdStanModel object. 3.3 Additional functions 3.3.1 NIG observations It is also possible to declare independent NIG observations y. The declaration is: model{ //observation layer--------------------------- y ~ nig_multi(etas, zetas, h); ... } where nig_multi has the signature: real nig_multi_lpdf(real etas, real mus, vector h) For the 1D version of the previous density use y ~ nig(...): real nig_lpdf(real x, real mean, real sigma, real etas, real mus, real h) 3.3.2 Sparse matrix computations To leverage on the sparsity of \\(\\mathbf{D}\\) we also built a function nig_model_2 which has the following signature: real nig_model_2_lpdf(vector X, matrix D, int[] Dv, int[] Du, int[] sizes, real etas, real mus, vector h, int compute_det) The new arguments are: Dv - Column indexes for the non-zero values in \\(\\mathbf{D}\\) Du - Indexes indicating where the row values start sizes - Array containing the number of rows, number of columns, and number of non-zero elements of \\(\\mathbf{D}\\) The arrays Dv, Du, and sizes should be built using Stan’s built-in functions for sparse matrix operations which use the compressed row storage format. Here is an example where \\(\\mathbf{D}(\\kappa)=\\kappa^2\\text{diag}(\\mathbf{h})+\\mathbf{G}\\): transformed data{ matrix[N,N] Graph = diag_matrix(h) + G; // Underlying graph (we can set kappa = 1) int sizew = rows(csr_extract_w(Graph)); // Number of non-zero values of matrix D int Dv[size(csr_extract_u(Graph))]; // Column indexes (in compressed row storage format) int Du[size(csr_extract_u(Graph))]; // Row indexes (in compressed row storage format) int sizes[3] = {N, N, sizeW}; // Vector containing number of rows, columns, and number of non-zero elements in D Dv = csr_extract_v(Graph); Du = csr_extract_u(Graph); } 3.4 Notes 3.4.1 Non-centered parameterization A non-centered parameterization takes advantage of the fact that: model{ //observation layer--------------------------- y ~ normal(B*beta + sigmax*x, sigmae); //latent field layer-------------------------- x ~ nig_model(D, etas, mus, h, 1); //log-pdf of Dx=Lambda, where Lambda is independent NIG noise } is equivalent to model{ //observation layer--------------------------- y ~ normal(B*beta + sigmax*inverse(D)*Lambda, sigmae); //latent field layer-------------------------- Lambda ~ nig_multi(etas, mus, h); //log-pdf of Lambda } where nig_multi yields the log-pdf of independent NIG noise. Both parameterizations are equal in distribution, but the latter enjoyx a nicer posterior geometry when the likelihood function is relatively diffuse, by removing explicit hierarchical correlations. This parameterization often leads to more efficient inference and it is discussed in Betancourt and Girolami (2015) for latent Gaussian models in Stan, and can also be found in Team (2020) and Betancourt (2020). inverse(D)*Lambda can be more efficiently implemented using mdivide_left_spd(D,Lambda) or mdivide_left_tri_low(D,Lambda) if \\(\\mathbf{D}\\) is symmetric positive definite or lower triangular. This model parameterization is worth keeping in mind, in case diagnostics reveal poor convergence or exploration of the HMC algorithm for hierarchical models. 3.4.2 Heavy-tailed distributions and Stan The NIG distribution converges to a Gaussian distribution when \\(\\eta\\to0\\) and to a Cauchy distribution when \\(\\eta\\to\\infty\\). The large extent of the heavy tails of the Cauchy distribution can be problematic in statistical computation. As described in Betancourt (2018) and Team (2020) the step size should be relatively large in the tail compared to the trunk, in order to explore the massive extent of the tails in a reasonable amount of time. However, with a large step size, there will be too much rejection in the central region of the distribution. The PC prior for \\(\\eta\\) helps mitigate this issue because it penalizes leptokurtosis and shrinks the NIG distribution towards to base Gaussian model. Also, the NIG distribution is semi-heavy-tailed, having exponentially decaying tails, which decay faster than the tails of the t-student distribution. Nonetheless, when the NIG distribution is close to the Cauchy limit it may be better to use the variance-mean mixture representation in eq. (2.3), which uses the conditional \\(\\mathbf{x}|\\mathbf{V}\\) which is Gaussian. 3.4.3 Determinant The matrices \\(\\mathbf{D}\\) that we will work with either do not depend on a model parameter, are lower triangular or symmetric positive definite. In the first case there is no need to compute the determinant in equation (3.1), since Stan does not need proportionality constants. In the second case, the determinant is the product of the diagonal elements. And in the final case we can compute the determinant based on the Cholesky decomposition: \\(\\log|\\mathbf{D}|=2\\sum_{i=1}^n\\log L_{ii}\\), where \\(\\mathbf{D} = \\mathbf{L} \\mathbf{L}^T\\) (this is done in nig_model and nig_model_2 when setting compude_det=1). Computing the log determinant using the Cholesky decomposition can still be slow, and in the first application 60% of the sampling time was spent computing log determinants. We will deal later with matrices \\(\\mathbf{D}\\) of the form \\(\\kappa^2\\mathbf{C}+\\mathbf{G}\\) and \\(\\mathbf{I}+\\rho\\mathbf{W}\\), where \\(\\mathbf{C}\\) is a diagonal matrix and \\(\\mathbf{G}\\) and \\(\\mathbf{W}\\) are symmetric. Consider the eigendecomposition of \\(\\mathbf{C}^{-1}\\mathbf{G} =\\Gamma \\mathbf{V}\\Gamma^{-1}\\), where \\(\\mathbf{V}=\\text{diag}(v_1,\\dotsc,v_n)\\) is a diagonal matrix containing the eigenvalues of \\(\\mathbf{C}^{-1}\\mathbf{G}\\). Then: \\[\\begin{align*} |\\kappa^2\\mathbf{C}+\\mathbf{G}| &amp;= |\\mathbf{C}||\\kappa^2+\\mathbf{C}^{-1}\\mathbf{G}| \\\\ &amp;= |\\mathbf{C}||\\Gamma(\\kappa^2 \\mathbf{I} + \\mathbf{V})\\Gamma^{-1}| \\\\ &amp;= |\\mathbf{C}||\\mathbf{\\Gamma}||\\kappa^2\\mathbf{I}+\\mathbf{V}||\\Gamma^{-1}| \\\\ &amp;= \\prod_{i=1}^n C_{ii}(\\kappa^2+v_i) \\end{align*}\\] Therefore \\(\\log |\\kappa^2\\mathbf{C}+\\mathbf{G}| \\propto \\sum_{i=1}^n\\log(\\kappa^2+v_i)\\), and one can compute the eigenvalues \\(v_i\\) only once before the HMC algorithm starts, and then evaluate \\(\\log \\mathbf{D}\\) efficiently using the previous result. Similar transformations can be applied when computing the determinant of \\(\\mathbf{D} = \\mathbf{I}+\\rho\\mathbf{W}\\), where now \\(\\log \\mathbf{D}=\\sum_{i=1}^n\\log(1-\\rho v_i)\\), and \\(v_i\\) are the eigenvalues of \\(\\mathbf{W}\\). References "],["simulations.html", "Chapter 4 Simulations 4.1 Libraries 4.2 Random Walk 1", " Chapter 4 Simulations The point of this section is to show that integrating out the mixing distributions \\(V_i\\) of the latent field \\(\\mathbf{x}\\) in the variance-mean mixture representation of (2.3) leads to a significant improvement in the computational efficiency of the HMC algorithm. More speed up is obtained when leveraging on sparse matrix computations and within-chain parallelization. We simulate data from a hierarchical model where the latent field follows an RW1 process with dimension \\(N \\in \\{2000,4000\\}\\). The only parameter to be estimated is the kurtosis parameter \\(\\eta\\). We fit the model using the implementations: 1. Variance-Mean Mixture: 1200 iterations and 1 chain 2. nig_model: 1200 iterations and 1 chain 3. nig_model_2: 1200 iterations and 1 chain with 4 threads per chain and utilizing sparse matrix computations The results are summarized in the following table where ESS is the effective sample size per second for the parameter \\(\\eta\\). Model Total time (s) ESS (Bulk) ESS (Tail) N=2000 Variance-Mean Mixture 266 0.10 0.10 N=2000 model_nig 278 1.41 2.11 N=2000 model_nig_2 77 5.30 9.56 N=4000 Variance-Mean Mixture 1132 0.01 0.04 N=4000 model_nig 884 0.32 0.54 N=4000 model_nig_2 113 2.25 4.79 Below we show the code 4.1 Libraries library(ggplot2) # More plots library(reshape2) library(Matrix) library(cmdstanr) # CmdStan R interface library(SuppDists) # Evaluate the density of a InvGauss distribution source(&quot;../files/utils.R&quot;) # Several utility functions options(mc.cores = parallel::detectCores()) 4.2 Random Walk 1 We simulate data from the following model: \\(\\mathbf{y}=\\text{N}(\\mathbf{x}, 0.7^2\\mathbf{I})\\), where \\(\\mathbf{D}_{AR1}\\mathbf{x}= \\mathbf{\\Lambda}\\). n &lt;- 2000 #number of data points m &lt;- 1 #rank deficiency of D sigmay &lt;- 0.7 # eta &lt;- 1 set.seed(123) V &lt;- rinvGauss(n,nu=1,lambda = 1/eta) #Mixing variables L &lt;- rnorm(n,0,sqrt(V)) #driving noise path &lt;- cumsum(L) + rnorm(n, 0, sigmay) #RW1 sample path + iid Gaussian noise df &lt;- data.frame(index = 1:n, path) df_melt = melt(df, id.vars = &#39;index&#39;) ggplot(df_melt, aes(x = index, y = value)) + geom_line(size=0.5) + theme(aspect.ratio = 0.5) 4.2.1 Fit with Variance-mean mixture representation We start by fitting the model with the variance-mean mixture representation. D &lt;- RW1.matrix(n) dat1 &lt;- list(N = n-m, Ny = n, y = path, h = rep(1,n-m), meanx = 1, D = as.matrix(D), sigmay = sigmay, thetaetas = 10, thetamus = 5) model_stan1 &lt;- cmdstan_model(&#39;../files/stan/benchmarking/NIGcond.stan&#39;) fit1 &lt;- model_stan1$sample(data = dat1, chains = 1, iter_warmup = 200, iter_sampling = 1000) fit1$save_object(&#39;../files/stan/benchmarking/fit1.rds&#39;) fit11 &lt;- readRDS(&quot;../files/stan/benchmarking/fit1.rds&quot;) knitr::kable(fit11$summary(c(&quot;etas&quot;)), &quot;simple&quot;, row.names = NA, digits=2) variable mean median sd mad q5 q95 rhat ess_bulk ess_tail etas 0.94 0.94 0.22 0.22 0.55 1.32 1.04 27.6 27.78 4.2.2 Fit with nig_model We now fit the same model with nig_model. model_stan2 &lt;- cmdstan_model(&#39;../files/stan/benchmarking/modelNIG.stan&#39;) fit2 &lt;- model_stan2$sample(data = dat1, chains = 1, iter_warmup = 200, iter_sampling = 1000) fit2$save_object(&#39;../files/stan/benchmarking/fit2.rds&#39;) fit2 &lt;- readRDS(&quot;../files/stan/benchmarking/fit2.rds&quot;) knitr::kable(fit2$summary(&quot;etas&quot;), &quot;simple&quot;, row.names = NA, digits=2) variable mean median sd mad q5 q95 rhat ess_bulk ess_tail etas 0.93 0.92 0.23 0.23 0.58 1.33 1 394.17 588.32 4.2.3 Fit with nig_model_2 We now fit the same model with nig_model_2, where we use sparse matrix computations and within-chain parallelization. model_stan3 &lt;- cmdstan_model(&#39;../files/stan/benchmarking/modelNIG2.stan&#39;, cpp_options = list(stan_threads = TRUE)) fit3 &lt;- model_stan3$sample(data = dat1, chains = 1, threads_per_chain = 4, iter_warmup = 200, iter_sampling = 1000) fit3$save_object(&#39;../files/stan/benchmarking/fit3.rds&#39;) fit3 &lt;- readRDS(&quot;../files/stan/benchmarking/fit3.rds&quot;) knitr::kable(fit3$summary(&quot;etas&quot;), &quot;simple&quot;, row.names = NA, digits=2) variable mean median sd mad q5 q95 rhat ess_bulk ess_tail etas 0.93 0.9 0.22 0.22 0.59 1.33 1 359.04 646.72 4.2.4 Fit with nig_model_2 Here we show the effective (bulk and tail) sample size per second for the different models fit11$time()$total ## [1] 266.8902 fit11$summary(&quot;etas&quot;)[&quot;ess_bulk&quot;]/fit11$time()$total ## ess_bulk ## 1 0.103418 fit11$summary(&quot;etas&quot;)[&quot;ess_tail&quot;]/fit11$time()$total ## ess_tail ## 1 0.1040842 fit2$time()$total ## [1] 278.6756 fit2$summary(&quot;etas&quot;)[&quot;ess_bulk&quot;]/fit2$time()$total ## ess_bulk ## 1 1.414447 fit2$summary(&quot;etas&quot;)[&quot;ess_tail&quot;]/fit2$time()$total ## ess_tail ## 1 2.111111 fit3$time()$total ## [1] 67.68172 fit3$summary(&quot;etas&quot;)[&quot;ess_bulk&quot;]/fit3$time()$total ## ess_bulk ## 1 5.304792 fit3$summary(&quot;etas&quot;)[&quot;ess_tail&quot;]/fit3$time()$total ## ess_tail ## 1 9.555367 "],["time-series.html", "Chapter 5 Time series 5.1 Autogressive process driven by NIG noise", " Chapter 5 Time series 5.1 Autogressive process driven by NIG noise One of the most simple and intuitive models to study time series data is the first-order autoregressive process driven by Gaussian noise, also denoted as AR(1). Here we use an AR(1) driven by NIG noise to model financial data. Heavier-tailed distributions for the noise can better capture extreme movements in the price of equities, which is crucial from a risk management perspective. This process is given by: \\[ x_t = \\rho x_{t-1} + \\sigma\\Lambda_t, \\] where \\(\\Lambda_t \\sim NIG(\\eta^\\star,\\zeta^\\star)\\) independent from \\(X_t\\). In our framework we have \\(\\mathbf{D}_{AR1}\\mathbf{x}=\\sigma\\mathbf{\\Lambda}(\\eta^\\star,\\zeta^\\star)\\), where: \\[ \\mathbf{D}_{AR1} = \\begin{pmatrix} \\sqrt{1-\\rho^2} &amp; &amp; &amp; &amp; &amp; \\\\ -\\rho &amp; 1 &amp; &amp; &amp; &amp; \\\\ &amp; -\\rho &amp; 1 &amp; &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; -\\rho &amp; 1 \\end{pmatrix}, \\] and the first element of the matrix is \\(\\sqrt{1-\\rho^2}\\) so that \\(x_t\\) is a stationary process. We study the end-of-day closing prices of the Google equity obtained from Yahoo finance from 31 December 2014 to 30 April 2021. This dataset was previously studied in Dhull and Kumar (2021) which used an autoregressive process of order 1 as suggested by the PACF plot. The previous authors used a conditional representation similar to eq. (2.1) and the auxiliary variables \\(V_i\\) to obtain MLE estimates for the parameters using the EM algorithm. However, this problem could have been turned into a simple optimization problem by using the joint density in (2.4) where it is possible to relate the data directly with the model parameters without needing the hidden variables \\(V_i\\). Since there is a clear positive trend in the data, we also add a drift parameter \\(c\\): \\[\\begin{equation} x_t = c +\\rho x_{t-1} + \\sigma\\Lambda_t, \\tag{5.1} \\end{equation}\\] 5.1.1 Libraries and data library(readr) # Read csv files library(cmdstanr) # CmdStan R interface library(posterior) # Process the output of cmdstanr sampling library(bayesplot) # Pair and trace plots library(ggplot2) # More plots library(SuppDists) # Evaluate density of a InvGauss distribution source(&quot;../files/utils.R&quot;) # Several utility functions options(mc.cores = parallel::detectCores()) goog &lt;- as.data.frame(read_csv(&quot;../files/data/GOOG.csv&quot;, col_names = TRUE)) data &lt;- goog$Close #closing prices N &lt;- length(data) mcmc_trace(as.data.frame(iter= 1:N, x = data)) 5.1.2 Priors The PC prior for \\(\\sigma\\) is an exponential distribution with some rate parameter \\(\\theta_\\sigma\\). To find this rate parameter we relate \\(\\sigma\\) with a more intuitive quantity which is the marginal standard deviation \\(\\sigma_{marg} = \\sigma/\\sqrt{1-\\rho^2}\\). As suggested in Simpson et al. (2017) we use the probability statement \\(P(\\sigma_{marg}&gt;U_\\sigma)=\\alpha_\\sigma\\), which leads to: \\[ \\theta_{\\sigma}(\\rho) = -\\frac{\\log(\\alpha_\\sigma)}{U_\\sigma\\sqrt{1-\\rho^2}} \\] If we consider \\(P(\\sigma_{marg}&gt;100)=0.1\\) then the log-density of the prior for \\(\\sigma\\) conditioned on \\(\\rho\\) is given by: thetas = 0.02/sqrt(1-rho) target += -log(thetas) - thetas*sigma The PC prior for \\(\\rho\\) was derived in Sørbye and Rue (2017). We consider the NIGAR model as an extension of a base model where there is no change in time (\\(\\rho=1\\)), and so the PC prior for \\(\\rho\\) has log-density: target += -sqrt(1 - rho)*theta - 0.5*log(1 - rho) As suggested in Sørbye and Rue (2017) is to find theta based on the probability statement \\(P(\\rho&gt;U_\\rho)=\\alpha_\\rho\\). The solution to this equation is given implicitly by: \\[ \\frac{1-\\exp (-\\theta \\sqrt{1-U_\\rho})}{1-\\exp (-\\sqrt{2} \\theta)}=\\alpha_\\rho \\] We consider \\(P(\\rho&gt;0.5)=0.75\\), which leads to the rate parameter \\(\\theta \\approx 1.55\\). As motivated in Cabral, Bolin, and Rue (2022) we used an exponential prior for \\(\\eta^\\star\\) and a Laplace prior for \\(\\zeta^\\star\\), both with different rate parameters. We scaled the PC priors for \\(\\eta^\\star\\) and \\(\\zeta^{star}\\) based on the following probability statements: \\(P(Q(\\eta^\\star)&gt;2)=0.1\\) and \\(P(|\\gamma|&gt;2)=0.1\\). \\(Q(\\eta^\\star)\\) quantifies how much more large events (larger than \\(3\\sigma\\)) the NIG noise has compared with Gaussian noise in the symmetric case (\\(\\zeta^\\star=0\\)): \\[Q(\\eta^\\star)=\\frac{P(|\\Lambda_i|&gt;3\\sigma)}{P(|Z_i|&gt;3\\sigma)}.\\] The probability statements lead to the rate parameters \\(\\theta_{\\eta^\\star}\\approx 15\\) and \\(\\theta_{\\zeta^\\star} \\approx 6.5\\). These priors are declared in the Stan model as follows: //prior for etas target += - theta_etas*etas; //prior for zetas target += - theta_zetas*fabs(zetas); Finally we use a \\(N(0,1)\\) prior for the drift parameter \\(c\\). 5.1.3 Gaussian fit The Stan model for the standard AR(1) model is: for (n in 2:N) x[n] ~ normal(c + rho*x[n-1], sigma); We need to pass to Stan the number of observations, the equity data and the rate parameters of the priors of \\(\\eta^\\star\\) and \\(\\zeta^\\star\\). dat1 &lt;- list(N = N, x = data, theta_etas = 15, theta_zetas = 6.5) model_stan_Gauss &lt;- cmdstan_model(&#39;../files/stan/ARGauss.stan&#39;) fit_Gauss &lt;- model_stan_Gauss$sample(data = dat1, chains = 4, iter_warmup = 500, iter_sampling = 3000) fit_Gauss$save_object(&quot;../files/fits/ARGauss.rds&quot;) fit_Gauss &lt;- readRDS(&quot;../files/fits/ARGauss.rds&quot;) knitr::kable(head(fit_Gauss$summary(),4), &quot;simple&quot;, row.names = NA, digits=2) variable mean median sd mad q5 q95 rhat ess_bulk ess_tail lp__ -5572.47 -5572.15 1.22 1.01 -5574.85 -5571.14 1 5711.61 7664.48 sigma 19.90 19.89 0.35 0.35 19.32 20.50 1 8367.34 8298.13 rho 1.00 1.00 0.00 0.00 1.00 1.00 1 6863.14 7401.70 c 1.89 1.87 0.55 0.54 1.01 2.82 1 7210.87 6149.14 The fit was successful with no wa rning m essages . The param eter \\(\\rho\\) is ver y close to 1, so the model is close to an RW1 model. 5.1.4 NIG fit Now, for the model declaration, we just need to change the following line of code. for (n in 2:N) x[n] ~ nig(c + rho*x[n-1], sigma, etas, zetas, 1); model_stan_NIG &lt;- cmdstan_model(&#39;../files/stan/ARNIG.stan&#39;) fit_NIG &lt;- model_stan_NIG$sample(data = dat1, chains = 4, iter_warmup = 200, iter_sampling = 1000) fit_NIG$save_object(&quot;../files/fits/ARNIG2.rds&quot;) #no warning fit_NIG &lt;- readRDS(&quot;../files/fits/ARNIG2.rds&quot;) knitr::kable(head(fit_NIG$summary(), 6), &quot;simple&quot;, row.names = NA, digits=2) variable mean median sd mad q5 q95 rhat ess_bulk ess_tail lp__ 411.34 411.64 1.69 1.63 408.23 413.51 1.00 1397.01 2089.07 sigma 947.99 942.74 90.48 91.82 811.70 1105.53 1.01 988.72 1618.74 rho 0.98 0.98 0.00 0.00 0.98 0.99 1.01 951.11 1770.60 etas 47.70 47.62 1.78 1.76 44.79 50.66 1.00 1985.15 1847.79 zetas 0.00 0.00 0.00 0.00 0.00 0.00 1.00 3984.35 2093.38 c 2.57 2.55 0.97 0.99 0.96 4.22 1.00 1438.07 1861.50 The fit was successful with no warning messages. The parameter \\(\\eta^\\star\\) is quite large (posterior mean of 48) indicating a clear heavy-tailed behavior for the noise, while the parameter \\(\\zeta^\\star\\) was close to 0, suggesting no need for an asymmetric distribution for the noise. 5.1.5 Comparizon We compare the Gaussian and NIG latent models using the leave-one-out cross-validation method of Vehtari, Gelman, and Gabry (2017), which gave unequivocal preference for the NIG model. fit_Gauss$loo() ## ## Computed from 12000 by 1592 log-likelihood matrix ## ## Estimate SE ## elpd_loo -7026.5 65.7 ## p_loo 6.4 1.3 ## looic 14053.0 131.5 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. fit_NIG$loo() ## ## Computed from 4000 by 1592 log-likelihood matrix ## ## Estimate SE ## elpd_loo 1228.0 2.8 ## p_loo 0.7 0.0 ## looic -2456.1 5.6 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. 5.1.6 Predictions We generate samples of the price distribution in the 3 days after the last recorded day based on eq. (5.1) for the Gaussian and NIG models. rho &lt;- as_draws_df(fit_Gauss$draws(&quot;rho&quot;))$rho c &lt;- as_draws_df(fit_Gauss$draws(&quot;c&quot;))$c sigma &lt;- as_draws_df(fit_Gauss$draws(&quot;sigma&quot;))$sigma n_draws &lt;- length(rho) x1594 = c + rho*data[N] + sigma*rnorm(n_draws) x1595 = c + rho*x1594 + sigma*rnorm(n_draws) x1596 = c + rho*x1595 + sigma*rnorm(n_draws) rho &lt;- as_draws_df(fit_NIG$draws(&quot;rho&quot;))$rho c &lt;- as_draws_df(fit_NIG$draws(&quot;c&quot;))$c etas &lt;- as_draws_df(fit_NIG$draws(&quot;etas&quot;))$etas zetas &lt;- as_draws_df(fit_NIG$draws(&quot;zetas&quot;))$zetas sigma &lt;- as_draws_df(fit_NIG$draws(&quot;sigma&quot;))$sigma n_draws &lt;- length(rho) NIG.x1594 = c + rho*data[N] + sigma*rNIG(n_draws, 0, 1, etas, zetas, h = 1) NIG.x1595 = c + rho*NIG.x1594 + sigma*rNIG(n_draws, 0, 1, etas, zetas, h = 1) NIG.x1596 = c + rho*NIG.x1595 + sigma*rNIG(n_draws, 0, 1, etas, zetas, h = 1) We compare next the distribution of the Gaussian and NIG predictions. The NIG predictions have heavier tails, and indicate a higher possibility of extreme price movements, compared with the Gaussian model, which underestimates the likelihood of such extreme events. bayesplot_grid( mcmc_hist(as.data.frame(iter = 1:n_draws, x=x1594)), mcmc_hist(as.data.frame(iter = 1:n_draws, x=NIG.x1594)), titles = c(&quot;1 step ahead prediction- Gaussian model&quot;, &quot;1 step ahead prediction - NIG model&quot;), xlim = c(2200,2600) ) bayesplot_grid( mcmc_hist(as.data.frame(iter = 1:n_draws, x=x1595)), mcmc_hist(as.data.frame(iter = 1:n_draws, x=NIG.x1595)), titles = c(&quot;2 step ahead prediction- Gaussian model&quot;, &quot;2 step ahead prediction - NIG model&quot;), xlim = c(2200,2600) ) bayesplot_grid( mcmc_hist(as.data.frame(iter = 1:n_draws, x=x1596)), mcmc_hist(as.data.frame(iter = 1:n_draws, x=NIG.x1596)), titles = c(&quot;3 step ahead prediction- Gaussian model&quot;, &quot;3 step ahead prediction - NIG model&quot;), xlim = c(2200,2600) ) References "],["spatial-matérn-model.html", "Chapter 6 Spatial Matérn model 6.1 Introduction 6.2 Implementation 6.3 Libraries and data 6.4 Discretization mesh 6.5 Data plot 6.6 Priors 6.7 Stan fit with a Gaussian model 6.8 Stan fit with a NIG driving noise 6.9 Leave-one-out cross validation 6.10 Prediction", " Chapter 6 Spatial Matérn model 6.1 Introduction A famous class of processes is spatial statistics are stationary Gaussian processes with Matérn covariance function (Matérn (1960)). Gaussian processes with this covariance function can be obtained as a solution to the SPDE equation (Lindgren, Rue, and Lindström (2011)): \\[\\begin{equation}\\label{eq:SPDE} (\\kappa^2 - \\Delta)^{\\alpha/2} X(\\mathbf{s}) = \\sigma \\mathcal{W}(\\mathbf{s}), \\ \\ \\mathbf{s} \\in \\mathbb{R}^d, \\end{equation}\\] where \\(\\kappa^2\\) is a spatial scale parameter, \\(\\Delta=\\sum_i \\partial^2/\\partial x_i^2\\) is the Laplace operator, \\(\\alpha\\) is a smoothness parameter, and \\(\\mathcal{W}(\\mathbf{s})\\) is a Gaussian white noise process. The approximation to discrete space in Lindgren, Rue, and Lindström (2011) uses the finite element method to the stochastic weak formulation of the previous SPDE. In 2D, it begins by expressing the process \\(X(\\mathbf{s})\\) as a sum of piecewise triangular basis functions: \\[ X(\\mathbf{s}) = \\sum_{i=1}^{n}w_i\\psi_i(\\mathbf{s}) \\] The spatial region of interest is partitioned into a set of \\(n\\) non-overlapping triangles creating a mesh, where each basis function \\(\\psi_i(\\mathbf{s})\\) takes the value 1 inside a given triangle and 0 outside. The weights \\(w_i\\) correspond to the process \\(X(\\mathbf{s})\\) at the nodes of the mesh. The distribution of the stochastic weights \\(\\mathbf{w}=[w_1,\\dotsc,w_n]^T\\) was found by the Galerkin method and leads to the system \\(\\mathbf{D}_\\alpha\\mathbf{w}=\\mathbf{Z}\\), where the Gaussian noise \\(Z_i\\) has variance \\(\\sigma^2h_i\\) with \\(h_i=\\int\\psi_i(\\mathbf{s})d\\mathbf{s}\\) being the area of the \\(i\\)th basis function. When \\(\\alpha=2\\), \\(\\mathbf{D}_2=\\kappa^{2} \\mathbf{C}+\\mathbf{G}\\), where: \\[\\mathbf{C}_{i j}=\\int\\psi_i(\\mathbf{s})\\psi_j(\\mathbf{s})d\\mathbf{s}, \\] \\[\\mathbf{G}_{i j}=\\int \\nabla\\psi_i(\\mathbf{s})\\nabla\\psi_j(\\mathbf{s})d\\mathbf{s}. \\] The mass-lumping technique is applied to approximate \\(\\mathbf{C}\\) with the diagonal matrix \\(\\text{diag}(\\mathbf{h})\\), so that the precision \\(\\mathbf{Q} = \\sigma^{-2}\\mathbf{D}_2 \\text{diag}(\\mathbf{h})^{-1}\\mathbf{D}_2\\) is sparse. The random field \\(X(\\mathbf{s})\\) at locations \\(\\mathbf{s}_1, \\mathbf{s}_2, \\dotsc\\), that is \\(\\mathbf{x}= [X(\\mathbf{s}_1), X(\\mathbf{s}_2), \\dotsc]^T\\), is then given by \\(\\mathbf{x}= \\mathbf{A}\\mathbf{w}\\), where \\(\\mathbf{A}\\) is the projector matrix with elements \\(A_{ij}=\\psi_i(s_j)\\). For even \\(\\alpha&gt;2\\) we have \\(\\mathbf{D}_{\\alpha}= \\mathbf{D}_2 \\mathbf{C}^{-1} \\mathbf{D}_{\\alpha-2}\\). All of these matrices can be computed using the function inla.mesh.fem() from the \\(INLA\\) package. Bolin (2014) extended the previous results to Type-G Matérn random fields by replacing the Gaussian noise process \\(\\mathcal{W}(\\mathbf{s})\\) with normal-variance mixture distributions, from which the NIG distribution is a special case. From the more flexible noise distribution we get a Matérn random field that can better model short-term variations and sharp peaks in the latent field, or “hotspots” and “coldspots”, that is, marginal events that would be considered extreme in a Gaussian model. It was shown that the stochastic weights now follow the system \\(\\mathbf{D}_\\alpha\\mathbf{w}=\\mathbf{\\Lambda}(\\eta^\\star,\\zeta^\\star)\\), where the matrix \\(\\mathbf{D}_\\alpha\\) is the same as in the Gaussian case seen before and \\(\\Lambda_i\\) is NIG noise with variance \\(\\sigma^2h_i\\), and flexibility parameters \\(\\eta^\\star\\) and \\(\\zeta^\\star\\). 6.2 Implementation We are going to implement a latent NIG driven Matérn random field to pressure data in the northwest region of North America. This dataset was previously studied in Bolin and Wallin (2020). The main point of this section is to show how to extend a Gaussian model to non-Gaussianity in Stan and discuss the potential benefits of using a non-Gaussian latent field, rather than implementing a realistic climate model. The full Stan models are in files\\stan\\GaussMaternSPDE.stan and files\\stan\\NIGMaternSPDE.stan and here we overview the main elements of the code. We consider no covariates for the temperature data \\(\\mathbf{y}\\) and the projector matrix \\(A\\) interpolates the random field that is being modeled at the mesh nodes on the measurement locations: \\[ \\mathbf{y}|\\mathbf{w} \\sim N(\\sigma\\mathbf{A}\\mathbf{w},\\sigma_\\epsilon) \\\\ \\mathbf{D}_2\\mathbf{w} = \\mathbf{\\Lambda}(\\eta^\\star,\\zeta^\\star), \\\\ \\ \\text{where} \\ \\mathbf{D}_2 = \\kappa^{2} \\text{diag}(\\mathbf{h})+\\mathbf{G} \\] We leave out the priors for now, which will be discussed later. 6.3 Libraries and data library(readr) # Read csv files library(INLA) # Compute discretization mesh and FEM matrices library(leaflet) # Interactive widgets library(leaflet.extras) # Fullscreen control for Leaflet widget library(cmdstanr) # CmdStan R interface library(posterior) # Process the output of cmdstanr sampling library(bayesplot) # Pair and trace plots library(ggplot2) # More plots library(GIGrvg) # Evaluate the density of a GIG distribution source(&quot;../files/utils.R&quot;) # Several utility functions options(mc.cores = parallel::detectCores()) The weatherdata.csv contains the temperature where the sample mean was subtracted from the data and the coordinates where the measurements were taken. weatherdata &lt;- as.data.frame(read_csv(&quot;../files/data/weatherdata.csv&quot;, col_names = TRUE)) 6.4 Discretization mesh Next, we create the triangle mesh to cover the studied region using the inla.mesh.2d function. The input is weatherdata[,c(\"lon\",\"lat\")] which are the coordinates where the measurements were taken, max.edge is the maximum allowed edge for each triangle, cutoff is the minimum allowed distance between vertexes, and max.n.strict is the maximum number of nodes allowed. From the mesh object, we obtain the matrix \\(\\mathbf{G}\\) which describes the connectivity of the mesh nodes, the vector \\(\\mathbf{h}\\) which contains the area of the triangles, and the projector matrix \\(\\mathbf{A}\\) which links the location of the mesh nodes to the location of the observations. The matrix \\(\\mathbf{C}\\) is just a diagonal matrix with the constants \\(h_i\\). mesh &lt;- inla.mesh.2d(loc = weatherdata[,c(&quot;lon&quot;,&quot;lat&quot;)], max.edge = c(1.5, 2.5), cutoff = 0.3, max.n.strict = 400) Ny &lt;- length(weatherdata$temp) #Number of observations N &lt;- mesh$n #Number of nodes fem &lt;- inla.mesh.fem(mesh, order=2) G &lt;- fem$g1 h &lt;- diag(fem$c0) A &lt;- inla.spde.make.A(mesh = mesh, loc = as.matrix(weatherdata[,c(&quot;lon&quot;,&quot;lat&quot;)])) 6.5 Data plot In the next Leaflet widget, we plot the data and the discretization mesh. We hid the Leaflet code here since it is quite long, but you can find it in the original Rmarkdown file. 6.6 Priors We use the default INLA prior for \\(\\sigma_\\epsilon\\). The penalized complexity priors for the scale parameter \\(\\sigma\\) and spatial range parameter \\(\\kappa\\) were derived in Fuglstad et al. (2019) and their log-likelihood can be defined in the model block as: ... //prior layer--------------------------------- //prior for sigmae sigmae ~ inv_gamma(1, 0.00005); //prior for rho and sigmam target += - log(kappa) - lambda1*kappa - lambda2*sigmax/kappa; ... With the PC prior approach, we get the distribution for the priors up to a scaling constant. The hyperparameters lambda1 and lambda2 are found by relating \\(\\sigma\\) and \\(\\kappa\\) with more interpretable parameters, namely the marginal standard deviation \\(\\sigma_{marg}\\) and the practical correlation range \\(\\rho=\\sqrt{8}/\\kappa\\) (the distance at which the correlation is approximately 0.1). Then lambda1 and lambda2 can be found by setting the probabilities \\(P(\\rho&lt;\\rho_0)=\\alpha\\) and \\(P(\\sigma_{marg}&gt;\\sigma_0)=\\alpha\\) as follows: # P(practic.range &lt; 0.1) = 0.01, base model range -&gt; infty range.U = 0.1 range.alpha = 0.01 # P(sigma &gt; 100) = 0.1 sigma.U = 100 sigma.alpha = 0.1 lambda1 = -log(range.alpha)*range.U/sqrt(8) lambda2 = -log(sigma.alpha)/(sigma.U*sqrt(4*pi)) The priors for the parameter \\(\\eta^\\star\\) and \\(\\zeta^\\star\\) are declared next: ... //prior layer--------------------------------- //prior for etas theta_etas = -4*log(alphaeta)*kappa^2; target += log(theta_etas) - theta_etas*etas; //prior for zetas target += - thetazetas*fabs(zetas); ... As motivated in Cabral, Bolin, and Rue (2022) we use an exponential distribution for \\(\\eta^\\star\\) with rate \\(\\theta_\\eta = 4\\log(\\alpha_{\\eta})\\kappa^2\\). The hyperparameter \\(\\alpha_{\\eta^\\star}\\) is the probability of having twice as much large marginal events compared with the Gaussian case: \\(P(Q(\\eta,\\kappa) &gt; 2)= \\alpha_{\\eta^\\star}\\), where: \\[ Q(\\eta,\\kappa) = \\frac{P(|X_{\\eta,\\kappa}(\\mathbf{s})|&gt;3\\sigma_{marg}(\\kappa))}{P(|X_{\\eta=0,\\kappa}(\\mathbf{s})|&gt;3\\sigma_{marg}(\\kappa))} , \\] We will use \\(\\alpha_{\\eta^\\star}==0.5\\), and for the Laplace prior on \\(\\zeta^\\star\\) we will use the rate parameter \\(\\theta_\\zeta = 13\\). 6.7 Stan fit with a Gaussian model Note that the Gaussian log-likelihood for \\(\\mathbf{w}\\) where \\(\\mathbf{D}\\mathbf{w}=\\mathbf{Z}\\), and \\(Z_i \\sim N(0,h_i)\\) is given by: \\[\\log\\pi(\\mathbf{x}) \\propto\\log |\\mathbf{D}| - 0.5\\sum_{i=1}^n [\\mathbf{D}\\mathbf{w}]_i^2/h_i.\\] We will be using a non-centered parameterization to improve the inference quality. To implement a non-centered parameterization we need to invert the matrix \\(\\mathbf{D} = \\kappa^2\\mathbf{C} + \\mathbf{G}\\) at every iteration, so we will leverage on the result: \\[ \\mathbf{D}^{-1} = \\mathbf{C}_1\\text{diag}(\\kappa^2+\\mathbf{v})^{-1}\\mathbf{C}_2 \\] where, \\[ \\mathbf{C}_1 = \\Gamma, \\ \\ \\ \\mathbf{C}_2 = (\\mathbf{C}\\Gamma)^{-1} \\] with \\(\\mathbf{\\Gamma}\\) and \\(\\mathbf{v}\\) being the eigenvector matrix and eigenvalue vector of \\(\\mathbf{C}^{-1}\\mathbf{G} = \\mathbf{\\Gamma}\\text{diag}(\\mathbf{v})\\mathbf{\\Gamma}^{-1}\\), respectively, which only need to be computed once, before the algorithm starts. Lastly, the representation we use is \\(\\mathbf{y}\\sim \\text{N}(\\mathbf{m}_\\mathbf{y},\\sigma_\\epsilon^2\\mathbf{I})\\), where the mean is \\(\\mathbf{m}_y=\\sigma_x\\mathbf{A}\\mathbf{w}\\), and the stochastic weights are \\(\\mathbf{w}=\\mathbf{D}^{-1}\\mathbf{\\Lambda}=\\mathbf{C}_1\\text{diag}(\\kappa^2+\\mathbf{v})^{-1}\\mathbf{C}_2 \\mathbf{Z}\\). The model statement is : model{ transformed parameters{ vector[N] w = diag_post_multiply(C1,inv(kappa^2+v))*C2*Z; //stochastic weights vector[Ny] my = sigmax*A*w; //mean of observations y } model{ real theta_etas; //response layer--------------------------- y ~ normal(my, sigmae); //latent field layer-------------------------- target += -0.5*sum(Z^2 ./ h); //log-density of i.i.d Gaussian vector z } Here we compute the matrices C1 and C2: C &lt;- Diagonal(N,h) e &lt;- eigen(solve(C)%*%G) eigenvalues &lt;- e$values eigenvectors &lt;- e$vectors C1 &lt;- eigenvectors C2 &lt;- solve(C%*%eigenvectors) v &lt;- eigenvalues We finally construct the list with all the required data. dat1 &lt;- list(N = N, Ny = Ny, y = weatherdata$temp, h = h, A = as.matrix(A), C1 = as.matrix(C1), C2 = as.matrix(C2), v = eigenvalues, lambda1 = lambda1, lambda2 = lambda2, alphaeta = 0.01, thetazetas = 13) Then we compile and fit the model. model_stan_Gauss &lt;- cmdstan_model(&#39;../files/stan/GaussMaternSPDE2.stan&#39;) fit_press_temp &lt;- model_stan_Gauss$sample(data = dat1, chains = 4, iter_warmup = 300, iter_sampling = 700, refresh = 10) fit_press_temp$save_object(&quot;../files/fits/fit_press_Gauss500.rds&quot;) Let us examine the summary of the fit for a few parameters. fit_Gauss &lt;- readRDS(&quot;../files/fits/fit_press_Gauss500.rds&quot;) knitr::kable(head(fit_Gauss$summary(),7), &quot;simple&quot;, row.names = NA, digits=2) variable mean median sd mad q5 q95 rhat ess_bulk ess_tail lp__ -938.96 -938.45 18.22 17.81 -969.40 -910.24 1.01 550.87 1057.93 sigmae 66.04 65.57 6.38 6.31 56.33 76.93 1.01 784.43 1636.17 sigmax 480.79 477.31 64.89 63.32 380.54 588.62 1.00 741.33 1378.68 kappa 0.60 0.59 0.12 0.12 0.42 0.82 1.00 936.83 1478.00 Lambda[1] 0.24 0.24 1.07 1.10 -1.46 1.97 1.00 3739.88 2255.28 Lambda[2] 0.31 0.32 1.04 1.05 -1.39 1.95 1.00 3831.08 1984.74 Lambda[3] 0.14 0.13 1.01 0.98 -1.56 1.86 1.00 5439.70 2144.62 All parameters had large effective sample sizes (ess_bulk and ess_ess_tail) and acceptable \\(\\hat{R}\\) values. Next, we have the pair and trace plots for \\(\\sigma_\\epsilon\\), \\(\\kappa\\), and \\(\\sigma\\). mcmc_pairs(fit_Gauss$draws(c(&quot;sigmae&quot;, &quot;sigmax&quot;, &quot;kappa&quot;)), diag_fun=&quot;dens&quot;, off_diag_fun=&quot;hex&quot;) mcmc_trace(fit_Gauss$draws(c(&quot;sigmae&quot;, &quot;sigmax&quot;, &quot;kappa&quot;)), facet_args = list(ncol = 2, strip.position = &quot;left&quot;)) 6.8 Stan fit with a NIG driving noise The model block for the NIG model is given next. We only changed the latent field layer which is now driven by i.i.d. NIG noise. transformed parameters{ vector[N] w = diag_post_multiply(C1,inv(kappa^2+v))*C2*Lambda; //stochastic weights vector[Ny] my = sigmax*A*w; //mean of observations y } model{ real theta_etas; //response layer--------------------------- y ~ normal(my, sigmae); //latent field layer-------------------------- Lambda ~ nig_multi(etas, zetas, h); //nig_multi returns the density of i.i.d. nig noise ... } We compile and fit the model next. model_stan_NIG &lt;- cmdstan_model(&#39;../files/stan/NIGMaternSPDE2.stan&#39;) fit_NIG_press &lt;- model_stan_NIG$sample(data = dat1, chains = 4, iter_warmup = 300, iter_sampling = 700, refresh = 10) fit_NIG_press$save_object(&quot;../files/fits/fit_press_500.rds&quot;) The summary shows acceptable \\(\\hat{R}\\) values and sample sizes, although compared to the Gaussian fit the effective sample sizes of \\(\\sigma_\\epsilon\\), \\(\\sigma\\), and \\(\\kappa\\) are lower. The posterior means of \\(\\sigma_\\epsilon\\) and \\(\\sigma_x\\) are smaller compared with the Gaussian model which means the predictions will have a smaller variance. The posterior mean of \\(\\kappa\\) is smaller indicating that the spatial correlation decays slower with distance. This is possibly caused by the peaks and short-term variations in the data, which are now better modeled by the latent field. The posterior distribution of \\(\\eta^\\star\\) indicates a clear departure from the Gaussian distribution, having a mean of 2.04. The 95% posterior quantiles of \\(\\zeta^\\star\\) include the 0, indicating insufficient evidence for asymmetry. fit_NIG &lt;- readRDS(&quot;../files/fits/fit_press_500.rds&quot;) knitr::kable(head(fit_NIG$summary(),7), &quot;simple&quot;, row.names = NA, digits=2) variable mean median sd mad q5 q95 rhat ess_bulk ess_tail lp__ -1233.87 -1235.16 29.63 29.41 -1281.61 -1184.46 1.01 264.81 600.53 sigmae 63.00 62.63 5.44 5.38 54.59 72.46 1.00 1447.13 1867.75 sigmax 336.77 332.75 52.60 53.75 257.84 427.72 1.00 520.13 837.36 kappa 0.37 0.36 0.08 0.08 0.25 0.50 1.01 473.43 1092.49 etas 2.04 1.84 1.07 0.94 0.70 4.05 1.02 234.62 779.20 zetas -0.01 0.00 0.06 0.05 -0.11 0.10 1.00 2226.78 1852.55 Lambda[1] 0.34 0.22 1.20 0.89 -1.37 2.43 1.00 2062.52 1017.63 mcmc_trace(fit_NIG$draws(c(&quot;sigmae&quot;, &quot;sigmax&quot;, &quot;kappa&quot;,&quot;etas&quot;,&quot;zetas&quot;)), facet_args = list(ncol = 2, strip.position = &quot;left&quot;)) Let us look at the posterior samples of the (standardized) latent field at some nodes. We can see at nodes 210 and 207 that in the NIG model the latent field has more mass in the tails and is more skewed. bayesplot_grid( mcmc_hist(fit_Gauss$draws(&quot;w[210]&quot;)), mcmc_hist(fit_NIG$draws(&quot;w[210]&quot;)), titles = c(&quot;Posterior of w[210] - Gaussian model&quot;, &quot;Posterior of w[1] - NIG model&quot;), xlim = c(-4,0) ) bayesplot_grid( mcmc_hist(fit_Gauss$draws(&quot;w[207]&quot;)), mcmc_hist(fit_NIG$draws(&quot;w[207]&quot;)), titles = c(&quot;Posterior of w[207] - Gaussian model&quot;, &quot;Posterior of w[94] - NIG model&quot;), xlim = c(0,1.5) ) 6.9 Leave-one-out cross validation We compare the Gaussian and NIG latent models using the leave-one-out cross-validation method of Vehtari, Gelman, and Gabry (2017). For these models, the elpd_loo estimates are not reliable. High Pareto-k values are often the result of model misspecification and typically correspond to data points considered outliers and surprising according to the model. We see that the NIG model has fewer observations with bad Pareto-k values. loo_Gauss &lt;- fit_Gauss$loo() loo_NIG &lt;- fit_NIG$loo() print(loo_Gauss) ## ## Computed from 2800 by 157 log-likelihood matrix ## ## Estimate SE ## elpd_loo -934.0 18.4 ## p_loo 74.6 11.8 ## looic 1868.0 36.7 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 67 42.7% 724 ## (0.5, 0.7] (ok) 48 30.6% 150 ## (0.7, 1] (bad) 35 22.3% 14 ## (1, Inf) (very bad) 7 4.5% 7 ## See help(&#39;pareto-k-diagnostic&#39;) for details. print(loo_NIG) ## ## Computed from 2800 by 157 log-likelihood matrix ## ## Estimate SE ## elpd_loo -924.0 21.4 ## p_loo 71.0 14.8 ## looic 1848.0 42.8 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 81 51.6% 385 ## (0.5, 0.7] (ok) 43 27.4% 234 ## (0.7, 1] (bad) 27 17.2% 7 ## (1, Inf) (very bad) 6 3.8% 1 ## See help(&#39;pareto-k-diagnostic&#39;) for details. 6.10 Prediction A common problem in spatial statistics is prediction, which in our context means finding the posterior distribution of the latent field \\(X(\\mathbf{s})\\) in locations where there are no measurement data. We will get posterior samples of \\(\\mathbf{x}_{pred}\\) which is the latent field \\(X(\\mathbf{s})\\) on a square grid spanning the studied region containing \\(100^2\\) nodes. First, we create the matrix coop containing the coordinates of the nodes in a square grid, and then inla.mesh.spde.make.A() creates the projector matrix \\(\\mathbf{A_p}\\) that links the original mesh nodes at which we modeled the latent field to the nodes of the new square grid. Finally, \\(\\mathbf{x}_{pred} = \\sigma \\mathbf{A_p} \\mathbf{w}\\), where we will use the posterior samples from \\(\\sigma\\) and \\(\\mathbf{w}\\) obtained in Stan to generate posterior samples of \\(\\mathbf{x}_{pred}\\). #Square grid containing 100^2 nodes n.grid &lt;- 100 coop &lt;- as.data.frame(expand.grid(seq(min(weatherdata$lon),max(weatherdata$lon),,n.grid), seq(min(weatherdata$lat),max(weatherdata$lat),,n.grid))) lenx &lt;- (max(weatherdata$lon) - min(weatherdata$lon))/n.grid leny &lt;- (max(weatherdata$lat) - min(weatherdata$lat))/n.grid #projector matrix for new grid Ap &lt;- inla.spde.make.A(mesh = mesh, loc = as.matrix(coop)) #Posterior samples from w W_NIG &lt;- as.matrix(as_draws_df(fit_NIG$draws(&quot;w&quot;))[,1:N]) #Posterior samples from sigma sigma_NIG &lt;- as.matrix(as_draws_df(fit_NIG$draws(&quot;sigmax&quot;)))[,1] #Posterior samples of w_pred W_NIG_pred &lt;- sigma_NIG * t(Ap %*% t(W_NIG)) We can visualize the posterior mean and standard deviation of \\(\\mathbf{x}_{pred}\\) in the following Leaflet widgets, as well as the probabilities that the latent field is larger than 600 and smaller than -400 in the prediction grid. It is possible to choose between two different map providers and we choose the same color scale in the Gaussian and NIG widgets. The posterior of the NIG latent field accurately captures the sharp peaks in the northeast region. There is a “hotpot” and “coldspot” close to one another and a Gaussian model over-smoothed these points. For instance, the largest pressure value is not covered by the 95% credible intervals of the Gaussian model, but it is covered in the NIG model. The NIG model predictions also have overall smaller standard deviations, especially in regions with few or no observations. If you hover the mouse on top of the circle markets (and deselect all projections), the observed value, posterior mean, and 95% credible intervals are shown. 6.10.0.1 NIG prediction 6.10.0.2 Gaussian prediction 6.10.1 Posterior distribution of \\(\\mathbf{V}\\oslash\\mathbf{h}\\) In the Gaussian case the vector \\(\\mathbf{V}\\oslash\\ \\mathbf{h}\\) is equal to \\(\\mathbf{1}\\), and so the plot of the posterior mean of \\(\\mathbf{V}\\oslash\\ \\mathbf{h}\\) can be used as a diagnostic tool to assess where departures from Gaussianity occur and if a non-Gaussian model is needed at all. We prefer looking at this diagnostic plot, rather than the posterior distribution of \\(\\eta^\\star\\). The function Vposterior in ..files\\utils.R can generate posterior samples of \\(\\mathbf{V}\\oslash\\ \\mathbf{h}\\) given posterior samples of \\(\\mathbf{w}\\), \\(\\kappa\\), \\(\\eta^\\star\\), and \\(\\zeta^\\star\\). We highlight the nodes where the 95% credible intervals of \\(\\mathbf{V}\\oslash\\ \\mathbf{h}\\) do not contain the value 1. Thus, the ´`tension’’ points of the Gaussian model where more flexibility is needed are revealed. kappa &lt;- as.matrix(as_draws_df(fit_NIG$draws(&quot;kappa&quot;)))[,1] etas &lt;- as.matrix(as_draws_df(fit_NIG$draws(&quot;etas&quot;)))[,1] zetas &lt;- as.matrix(as_draws_df(fit_NIG$draws(&quot;zetas&quot;)))[,1] V &lt;- Vposterior(W_NIG, kappa, G, etas, zetas, h) Vmean &lt;- colMeans(V) #posterior means of V/h sel &lt;- 1-((apply(V,2,quantile,0.05) &lt; 1) &amp; (apply(V,2,quantile,0.95) &gt; 1))*1 #do 95% quantiles include the value 1? In the following widget we can visualize the posterior mean of \\(\\mathbf{V}\\oslash\\mathbf{h}\\). References "],["sar-and-car-models.html", "Chapter 7 SAR and CAR models 7.1 SAR models 7.2 Columbus dataset and model 7.3 CAR models", " Chapter 7 SAR and CAR models In this section, we model aerial data, which is data that occurs on a lattice or an irregular grid with a countable set of locations or nodes. The two most common models for aerial data are conditional autoregressive (CAR) and simultaneous autoregressive (SAR) models, both known for having sparse precision matrices. For a review of these models see Hooten, Ver Hoef, and Hanks (2019), Riebler et al. (2016), and for the relationship between them see Ver Hoef, Hanks, and Hooten (2018). 7.1 SAR models Here we consider a SAR model built from the following relationship: \\[ \\mathbf{x} = \\mathbf{B}\\mathbf{x} + \\sigma\\mathbf{Z}, \\] where each element of the random vector \\(\\mathbf{x}\\) corresponds to a node in the lattice and \\(\\mathbf{Z}\\) is a vector of i.i.d. standard Gaussian noise. The matrix \\(\\mathbf{B}\\) causes simultaneous autoregressions of each random variable on its neighbors, where two regions are considered to be neighbors if they share a common border. Hence, SAR models are typically added as spatially structured random effects in hierarchical models in many fields such as disease mapping, ecology, and econometrics. The diagonal elements of \\(\\mathbf{B}\\) are 0 so that each node does not depend on itself. For simplicity, we assume \\(\\mathbf{B}=\\rho\\mathbf{W}\\), where \\(\\mathbf{W}\\) is a row standardized adjacency matrix and \\(-1&lt;\\rho&lt;1\\) so that the resulting precision matrix is valid. We find that this parameterization is more interpretable and there is an analog with an AR(1) process. In an AR(1) process, the value of the process at a given time point is \\(\\rho\\) multiplied by the value of the process at the previous time point plus Gaussian noise. In our SAR model, the value of a node is given by \\(\\rho\\) multiplied by the average of the values of the neighbors plus Gaussian noise. We end up with the system \\(\\mathbf{D}_{SAR}\\mathbf{x} = \\sigma\\mathbf{Z}\\), where \\(\\mathbf{D}_{SAR}=\\mathbf{I}-\\rho\\mathbf{W}\\). The equivalent model driven by NIG noise is then \\(\\mathbf{D}_{SAR}\\mathbf{x} = \\sigma\\mathbf{\\Lambda}\\), where \\(\\mathbf{\\Lambda}\\) is i.i.d. standardized NIG noise with parameters \\(\\eta^\\star\\) and \\(\\zeta^\\star\\). In this application, we will use the prior \\(\\rho \\sim \\mathcal{Unif}(0,1)\\), where we restrict the model so that it only accounts for positive spatial dependence. 7.1.1 Libraries library(spdep) # Columbus dataset library(INLA) library(sp) # Create Spatial polygons object for plotting library(rgdal) # Read polygon data library(leaflet) # Interactive widgets library(leaflet.extras) # Fullscreen control for Leaflet widget library(cmdstanr) # CmdStan R interface library(posterior) # Process the output of cmdstanr sampling library(bayesplot) # Pair and trace plots library(ggplot2) # More plots library(GIGrvg) # Evaluate density of a GIG distribution source(&quot;../files/utils.R&quot;) # Several utility functions options(mc.cores = parallel::detectCores()) 7.2 Columbus dataset and model The dataset consists of crime rates in thousands (\\(y_i\\)) in 49 counties of Columbus, Ohio, and can be found in the spdep R package. We will fit the following model: \\[ \\mathbf{y}_{i}= \\beta_0 + \\beta_1 \\text{HV}_i + \\beta_2 \\text{HI}_i + \\sigma_{\\mathbf{x}}\\mathbf{x}_i, \\] where \\(\\text{HV}_i\\) and \\(\\text{HI}_i\\) are the average household value and household income for county \\(i\\), \\(\\mathbf{x}\\) is a spatial effects SAR model, and \\(\\boldsymbol{\\epsilon}\\) are Gaussian heterogeneous effects. data(columbus) data &lt;- columbus[,c(&quot;CRIME&quot;,&quot;HOVAL&quot;,&quot;INC&quot;)] # data N &lt;- nrow(data) # number of counties map &lt;- readOGR(system.file(&quot;shapes/columbus.shp&quot;, package=&quot;spData&quot;)[1]) # shape file containing the polygons ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\R\\library\\spData\\shapes\\columbus.shp&quot;, layer: &quot;columbus&quot; ## with 49 features ## It has 20 fields ## Integer64 fields read as strings: COLUMBUS_ COLUMBUS_I POLYID In the next Leaflet widget, we show the crime rates and the two covariates. We obtain next the adjacency matrix \\(\\mathbf{W}\\). nb_q &lt;- poly2nb(map) # Construct neighbours list from polygon list nb_W &lt;- nb2listw(nb_q, style=&quot;B&quot;, zero.policy=TRUE) # Spatial weights for neighbours lists W &lt;- as.matrix(as(nb_W, &quot;sparseMatrix&quot;)) # Adjacency matrix W W &lt;- diag(1/rowSums(W))%*%W # Row standardize adjacency matrix v &lt;- eigen(W)$values # Eigenvalues of adjacency matrix The data list passed to Stan is the following: B &lt;- cbind(rep(1,N),data[,c(2,3)]) #Design matrix dat1 &lt;- list(N = N, N_covariates = 3, y = data$CRIME, B = B, W = W, v = eigen(W)$values, #Eigenvalues of W thetaetas = 5, thetazetas = 4) 7.2.1 Gaussian fit Note that the Gaussian log-likelihood for \\(\\mathbf{x}\\) where \\(\\mathbf{D}\\mathbf{x}=\\mathbf{Z}\\) and \\(Z_i \\overset{i.i.d.}{\\sim} N(0,1)\\) is given by: \\[\\log\\pi(\\mathbf{x}) \\propto\\log |\\mathbf{D}| - 0.5\\sum_{i=1}^n [\\mathbf{D}\\mathbf{w}]_i^2.\\] We declare the spatial effects \\(\\mathbf{x}\\) in the transformed parameters and then use the previous log density in the model block. The determinant of \\(\\mathbf{D}\\) is \\(\\sum_i \\log(1-\\rho v_i)\\), where the \\(v_i\\)’s are the eigenvalues of \\(\\mathbf{W}\\). transformed parameters{ vector[N] X = (y - B*beta)/sigma; // Spatial effects } model{ matrix[N,N] D = add_diag(-rho*W, 1); // D = I - rho W; target += -0.5*dot_self(D*X); target += sum(log(1-rho*v)); // Log determinant of D ... } model_stan_Gauss &lt;- cmdstan_model(&#39;../files/stan/GaussSAR2.stan&#39;) fit_Gauss &lt;- model_stan_Gauss$sample(data = dat1, chains = 4, iter_warmup = 1000, iter_sampling = 1000) fit_Gauss$save_object(&quot;../files/fits/fit_columbus_Gauss2.rds&quot;) fit_Gauss$cmdstan_diagnose() #No warnings Let us look at the summary: fit_Gauss &lt;- readRDS(&quot;../files/fits/fit_columbus_Gauss2.rds&quot;) knitr::kable(head(fit_Gauss$summary(),6), &quot;simple&quot;, row.names = NA, digits=2) variable mean median sd mad q5 q95 rhat ess_bulk ess_tail lp__ -16.41 -16.02 1.78 1.55 -19.83 -14.27 1 1630.88 2010.24 sigma 20.50 20.29 2.65 2.57 16.50 25.20 1 2986.27 2481.06 rho 0.36 0.36 0.19 0.21 0.05 0.67 1 1901.94 1251.32 beta[1] 63.56 63.67 10.76 10.35 45.36 80.69 1 2315.12 2221.33 beta[2] -0.30 -0.29 0.19 0.19 -0.62 0.02 1 3094.38 2327.45 beta[3] -1.19 -1.20 0.70 0.69 -2.34 -0.05 1 2080.53 2148.89 7.2.2 NIG fit The SAR model driven with NIG noise can be declared in Stan by just changing one line of code of the Gaussian Stan model. We set the last argument of modelNIG to 0 so that the determinant is not computed twice. model{ matrix[N,N] D = add_diag(-rho*W, 1); X ~ modelNIG(D, etas, zetas, h, 0); target += sum(log(1-rho*v)); // Log determinant of D ... } model_stan_NIG &lt;- cmdstan_model(&#39;../files/stan/NIGSAR2.stan&#39;) fit_NIG &lt;- model_stan_NIG$sample(data = dat1, chains = 4, iter_warmup = 1000, iter_sampling = 1000, max_treedepth = 20, adapt_delta = 0.999) fit_NIG$save_object(&quot;../files/fits/fit_columbus_NIG2.rds&quot;) fit_NIG$cmdstan_diagnose() #4 out of 4000 iterations diverged The posterior distributions for the regression coefficients were similar, and the posterior distributions of \\(\\eta^\\star\\) and \\(\\zeta^\\star\\) suggest some heavy-tailedness although no asymmetry. fit_NIG &lt;- readRDS(&quot;../files/fits/fit_columbus_NIG2.rds&quot;) knitr::kable(head(fit_NIG$summary(),8), &quot;simple&quot;, row.names = NA, digits=2) variable mean median sd mad q5 q95 rhat ess_bulk ess_tail lp__ -62.05 -61.75 2.46 2.43 -66.55 -58.62 1 951.97 1544.98 sigma 22.28 22.09 3.03 3.03 17.60 27.56 1 1998.27 1908.75 rho 0.38 0.39 0.19 0.22 0.06 0.71 1 2003.53 1421.08 etas 0.84 0.75 0.54 0.52 0.11 1.86 1 1837.81 1228.51 zetas -0.03 -0.01 0.32 0.16 -0.56 0.45 1 2105.61 1162.45 beta[1] 63.67 63.65 10.33 9.82 46.74 80.45 1 1874.01 2060.93 beta[2] -0.24 -0.23 0.19 0.19 -0.57 0.07 1 1771.66 2069.54 beta[3] -1.37 -1.36 0.67 0.67 -2.46 -0.26 1 1644.77 2231.99 The posterior draws for \\(\\mathbf{V}\\) can be generated based on eq. (2.5) and from the posterior draws of \\(\\mathbf{x}\\), \\(\\eta^\\star\\), \\(\\zeta^\\star\\) and \\(\\rho\\) as done next. We utilized the function VposteriorSAR located at ../files/utils.R. High values for \\(V_i\\) indicate that region \\(i\\) has a spike in the spatial effects, that were not captured by the covariates, and are usually too large to be modeled by a Gaussian distribution. X &lt;- as.matrix(as_draws_df(fit_NIG$draws(&quot;X&quot;)))[,1:N] etas &lt;- as.matrix(as_draws_df(fit_NIG$draws(&quot;etas&quot;)))[,1] zetas &lt;- as.matrix(as_draws_df(fit_NIG$draws(&quot;zetas&quot;)))[,1] rho &lt;- as.matrix(as_draws_df(fit_NIG$draws(&quot;rho&quot;)))[,1] h &lt;- rep(1,N) V_post &lt;- VposteriorSAR(X, rho, W, etas, zetas, h) In the next leaflet widget, we show the posterior mean and standard deviation of the spatial effects as well as the posterior mean of \\(\\mathbf{V}\\). 7.3 CAR models ICAR stands for the intrinsic conditional autoregressive process. The word “conditional” on ICAR is because the distribution of each node is defined conditionally on the values of the neighboring nodes: \\[ x_i|\\mathbf{x_{-i}} \\sim \\mathcal{N}\\left(\\frac{1}{n_i}\\sum_{j:j\\sim i}x_j,\\frac{\\sigma^2}{n_i}\\right), \\] where \\(\\mathbf{x_{-i}}\\) is a vector containing all \\(z_j\\) except \\(j \\neq i\\), \\(j \\sim i\\) denotes the set of all unordered pairs of neighbors and \\(n_i\\) is the number of neighbors of node \\(i\\). The previous conditional structure leads to a GMRF with a precision matrix with a rank deficiency of 1, given by: \\[ Q_{i j}=\\left\\{\\begin{array}{ll} -1, &amp; i \\text { is neighboring } j \\\\ n_i, &amp; i=j \\\\ 0, &amp; \\text { else. } \\end{array}\\right. \\] In Rue and Held (2005) this model was constructed based on the assumption of ‘independent’ increments: \\[ x_i-x_j \\sim \\mathcal{N}(0, \\sigma^2), \\] where independent is written in brackets due to (hidden) linear constraints imposed by the more complicated geometry. The log probability density of \\(\\mathbf{x}\\) for \\(\\sigma=1\\) is given by: \\[ \\log\\pi(\\mathbf{x}) \\propto -\\frac{1}{2}\\sum_{i \\sim j}(x_i-x_j)^2 \\] For the NIG model extension, we assume that the ‘independent’ increments follow the more flexible NIG distribution. 7.3.1 Dataset and model We will model the Scotland Lip cancer dataset using the previous ICAR model for the spatial effects. The Gaussian model implementation was taken from Morris (2019) who studied several parameterizations of this model and their implementation on Stan. Here we consider only the BYM2 parameterization (see Riebler et al. (2016)). The observed number of cancer cases at location \\(i\\) is given by: \\[y_i \\sim \\text{Poisson}(E_i \\ RR_i)\\] \\[\\log RR_i = \\psi_i = x\\beta + \\sigma(\\sqrt{1-\\rho}\\theta + \\sqrt{\\rho}\\phi), \\] where: \\(E_i\\) is the expected count \\(RR_i\\) is the relative risk at area \\(i\\) \\(x\\) is the design matrix \\(\\beta\\) are the regression coefficients \\(\\sigma\\) is the overall standard deviation \\(\\phi\\) is an ICAR spatial component \\(\\theta\\) are Gaussian heterogeneous effects \\(\\rho\\in[0,1]\\) measures how much of the variability comes from the ICAR component For an explanation of the Stan implementation see Morris (2019). In this application, \\(x\\) only includes one covariate that indicates the proportion of the population engaged in agriculture, fishing, or forestry (AFF). source(&quot;../files/data/scotland_data.R&quot;) y = data$y; E = data$E; K = 1; x = 0.1 * data$x; nbs = mungeCARdata4stan(data$adj, data$num); N = nbs$N; node1 = nbs$node1; node2 = nbs$node2; N_edges = nbs$N_edges; #Build the adjacency matrix using INLA library functions adj.matrix = sparseMatrix(i=nbs$node1,j=nbs$node2,x=1,symmetric=TRUE) #The ICAR precision matrix (note! This is singular) Q= Diagonal(nbs$N, rowSums(adj.matrix)) - adj.matrix #Add a small jitter to the diagonal for numerical stability (optional but recommended) Q_pert = Q + Diagonal(nbs$N) * max(diag(Q)) * sqrt(.Machine$double.eps) # Compute the diagonal elements of the covariance matrix subject to the # constraint that the entries of the ICAR sum to zero. #See the inla.qinv function help for further details. Q_inv = inla.qinv(Q_pert, constr=list(A = matrix(1,1,nbs$N),e=0)) #Compute the geometric mean of the variances, which are on the diagonal of Q.inv scaling_factor = exp(mean(log(diag(Q_inv)))) dat1 &lt;- list(N = N, N_edges = N_edges, node1 = node1, node2 = node2, y = y, K = 1, x = as.matrix(x=x), E = E, scaling_factor = scaling_factor, theta_etas = 10, theta_zetas = 5) model_stan_Gauss &lt;- cmdstan_model(&#39;../files/stan/GaussPoissonCAR2.stan&#39;) fit_Gauss &lt;- model_stan_Gauss$sample(data = dat1, chains = 4, iter_warmup = 5000, iter_sampling = 1000) fit_Gauss$save_object(&quot;../files/fits/fit_scotland_Gauss2.rds&quot;) fit_Gauss$cmdstan_diagnose() fit_Gauss &lt;- readRDS(&quot;../files/fits/fit_scotland_Gauss2.rds&quot;) knitr::kable(head(fit_Gauss$summary(),5), &quot;simple&quot;, row.names = NA, digits=2) variable mean median sd mad q5 q95 rhat ess_bulk ess_tail lp__ 751.08 751.32 8.97 8.79 736.31 765.44 1.01 1120.53 2132.45 beta0 -0.21 -0.21 0.13 0.13 -0.41 0.00 1.00 2398.66 2957.29 betas[1] 0.36 0.36 0.13 0.13 0.14 0.57 1.00 2215.25 2441.26 sigma 0.52 0.51 0.08 0.08 0.39 0.67 1.00 1109.89 1852.48 rho 0.87 0.92 0.14 0.10 0.57 1.00 1.01 515.40 1101.00 7.3.2 NIG fit For the NIG implementation we replace the Gaussian log-likelihood for the increments: target += -0.5 * dot_self(phi[node1] - phi[node2]); by the NIG log-likelihood: for(i in 1:N_edges){ ll[i] = nig_lpdf( phi[node1[i]] - phi[node2[i]] | 0, 1, etas, zetas, 1);} target += sum(ll); model_stan_NIG &lt;- cmdstan_model(&#39;../files/stan/NIGPoissonCAR2.stan&#39;) fit_NIG &lt;- model_stan_NIG$sample(data = dat1, chains = 4, iter_warmup = 5000, iter_sampling = 1000) fit_NIG$save_object(&quot;../files/fits/fit_scotland_NIG2.rds&quot;) fit_NIG$cmdstan_diagnose() #No warnings We compare now the posterior summaries of the NIG model with the Gaussian model. We observe a higher posterior mean for \\(\\rho\\) which indicates that more of the variability comes from the spatially structured effects and less from the spatially unstructured effects, and suggests that the more flexible model for the spatial structure effects could better capture the neighboring relationships in the lattice. The posterior distribution of \\(\\beta\\) stills indicates a positive relationship between the covariate AFF (proportion of the population engaged in agriculture, fishing, or forestry) and lip cancer risk, but the posterior mean dropped from 0.36 to 0.26. fit_NIG &lt;- readRDS(&quot;../files/fits/fit_scotland_NIG2.rds&quot;) knitr::kable(head(fit_NIG$summary(),5), &quot;simple&quot;, row.names = NA, digits=2) variable mean median sd mad q5 q95 rhat ess_bulk ess_tail lp__ 655.39 655.93 9.49 9.38 638.65 669.87 1.00 861.73 1620.21 beta0 -0.15 -0.15 0.11 0.11 -0.33 0.04 1.00 1343.49 2444.18 betas[1] 0.27 0.28 0.12 0.12 0.07 0.46 1.00 1227.03 2099.47 sigma 0.65 0.65 0.10 0.10 0.50 0.83 1.01 631.94 1170.46 rho 0.98 0.99 0.04 0.01 0.90 1.00 1.00 1084.00 1791.75 mcmc_pairs(fit_NIG$draws(c(&quot;sigma&quot;, &quot;rho&quot;, &quot;betas[1]&quot;, &quot;etas&quot;,&quot;zetas&quot;)), diag_fun=&quot;dens&quot;, off_diag_fun=&quot;hex&quot;) mcmc_trace(fit_NIG$draws(c(&quot;sigma&quot;, &quot;rho&quot;, &quot;betas[1]&quot;, &quot;etas&quot;,&quot;zetas&quot;)), facet_args = list(ncol = 2, strip.position = &quot;left&quot;)) 7.3.3 Comparizon We compared both models using the leave-one-out cross-validation estimates with the Pareto smoothed importance sampling (PSIS) algorithm in Stan. The looic estimate prefers the NIG model, however, this estimate may not be reliable since some Pareto \\(k\\) diagnostic values are too high. loo1 &lt;- fit_Gauss$loo(save_psis = TRUE) loo2 &lt;- fit_NIG$loo(save_psis = TRUE) loo1[[1]] ## Estimate SE ## elpd_loo -152.42917 5.659457 ## p_loo 26.05334 2.782331 ## looic 304.85834 11.318914 loo2[[1]] ## Estimate SE ## elpd_loo -140.03926 5.287102 ## p_loo 17.18549 2.055486 ## looic 280.07852 10.574204 The Pareto \\(k\\) diagnostic values are shown next, which are used to assess the reliability of the estimates. The estimates of the tail shape \\(k\\) are high for several locations and cross the value 1 at locations 15 (NE.fife) and 22 (Aberdeen) for the Gaussian model suggesting that the variance of the raw importance ratios is infinite. High estimates of the tail shape parameter \\(k\\) indicate that the full posterior is not a good importance sampling approximation to the leave-one-out posterior. Thus, the observations are surprising according to the model. This is more likely to happen in a non-robust model with highly influential observations. The mode robust NIG model may reduce the sensitivity with regards to influential observation and it has smaller tail shapes. plot(loo1, label_points = TRUE, main = &quot;Gaussian&quot;) loo1$psis_object ## Computed from 4000 by 56 log-weights matrix ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 17 30.4% 439 ## (0.5, 0.7] (ok) 24 42.9% 241 ## (0.7, 1] (bad) 13 23.2% 69 ## (1, Inf) (very bad) 2 3.6% 14 ## See help(&#39;pareto-k-diagnostic&#39;) for details. plot(loo2, label_points = TRUE, main = &quot;NIG&quot;) loo2$psis_object ## Computed from 4000 by 56 log-weights matrix ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 29 51.8% 609 ## (0.5, 0.7] (ok) 14 25.0% 431 ## (0.7, 1] (bad) 13 23.2% 63 ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## See help(&#39;pareto-k-diagnostic&#39;) for details. We generate posterior samples from V next, using eq. eq. (2.5): X &lt;- as.data.frame(as_draws_df(fit_NIG$draws(&quot;eta&quot;))[,1:N]) etas &lt;- as_draws_df(fit_NIG$draws(&quot;etas&quot;))$etas zetas &lt;- as_draws_df(fit_NIG$draws(&quot;zetas&quot;))$zetas V &lt;- VposteriorCAR(X, node1, node2, etas, zetas) V &lt;- colMeans(V) 7.3.4 NIG model - Relative risk In the following Leaflet widget, we plot the mean of the relative risk, and the probability that the relative risk in a given area is larger than 3 \\(P(RR&gt;3)\\). We plot also the posterior mean of \\(V_i\\) for each edge on the graph. ## code note ## 2496 27700 OSGB 1936 / British National Grid ## prj4 ## 2496 +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs ## prj_method ## 2496 Transverse Mercator ## [1] &quot;skye-lochalsh&quot; &quot;banff-buchan&quot; &quot;caithness&quot; &quot;berwickshire&quot; ## [5] &quot;ross-cromarty&quot; &quot;orkney&quot; &quot;moray&quot; &quot;shetland&quot; ## [9] &quot;lochaber&quot; &quot;gordon&quot; &quot;western.isles&quot; &quot;sutherland&quot; ## [13] &quot;nairn&quot; &quot;wigtown&quot; &quot;NE.fife&quot; &quot;kincardine&quot; ## [17] &quot;badenoch&quot; &quot;ettrick&quot; &quot;inverness&quot; &quot;roxburgh&quot; ## [21] &quot;angus&quot; &quot;aberdeen&quot; &quot;argyll-bute&quot; &quot;clydesdale&quot; ## [25] &quot;kirkcaldy&quot; &quot;dunfermline&quot; &quot;nithsdale&quot; &quot;east.lothian&quot; ## [29] &quot;perth-kinross&quot; &quot;west.lothian&quot; &quot;cumnock-doon&quot; &quot;stewartry&quot; ## [33] &quot;midlothian&quot; &quot;stirling&quot; &quot;kyle-carrick&quot; &quot;inverclyde&quot; ## [37] &quot;cunninghame&quot; &quot;monklands&quot; &quot;dumbarton&quot; &quot;clydebank&quot; ## [41] &quot;renfrew&quot; &quot;falkirk&quot; &quot;clackmannan&quot; &quot;motherwell&quot; ## [45] &quot;edinburgh&quot; &quot;kilmarnock&quot; &quot;east.kilbride&quot; &quot;hamilton&quot; ## [49] &quot;glasgow&quot; &quot;dundee&quot; &quot;cumbernauld&quot; &quot;bearsden&quot; ## [53] &quot;eastwood&quot; &quot;strathkelvin&quot; &quot;tweeddale&quot; &quot;annandale&quot; 7.3.5 Relative risk - Gaussian model ## code note ## 2496 27700 OSGB 1936 / British National Grid ## prj4 ## 2496 +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs ## prj_method ## 2496 Transverse Mercator ## [1] &quot;skye-lochalsh&quot; &quot;banff-buchan&quot; &quot;caithness&quot; &quot;berwickshire&quot; ## [5] &quot;ross-cromarty&quot; &quot;orkney&quot; &quot;moray&quot; &quot;shetland&quot; ## [9] &quot;lochaber&quot; &quot;gordon&quot; &quot;western.isles&quot; &quot;sutherland&quot; ## [13] &quot;nairn&quot; &quot;wigtown&quot; &quot;NE.fife&quot; &quot;kincardine&quot; ## [17] &quot;badenoch&quot; &quot;ettrick&quot; &quot;inverness&quot; &quot;roxburgh&quot; ## [21] &quot;angus&quot; &quot;aberdeen&quot; &quot;argyll-bute&quot; &quot;clydesdale&quot; ## [25] &quot;kirkcaldy&quot; &quot;dunfermline&quot; &quot;nithsdale&quot; &quot;east.lothian&quot; ## [29] &quot;perth-kinross&quot; &quot;west.lothian&quot; &quot;cumnock-doon&quot; &quot;stewartry&quot; ## [33] &quot;midlothian&quot; &quot;stirling&quot; &quot;kyle-carrick&quot; &quot;inverclyde&quot; ## [37] &quot;cunninghame&quot; &quot;monklands&quot; &quot;dumbarton&quot; &quot;clydebank&quot; ## [41] &quot;renfrew&quot; &quot;falkirk&quot; &quot;clackmannan&quot; &quot;motherwell&quot; ## [45] &quot;edinburgh&quot; &quot;kilmarnock&quot; &quot;east.kilbride&quot; &quot;hamilton&quot; ## [49] &quot;glasgow&quot; &quot;dundee&quot; &quot;cumbernauld&quot; &quot;bearsden&quot; ## [53] &quot;eastwood&quot; &quot;strathkelvin&quot; &quot;tweeddale&quot; &quot;annandale&quot; References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
