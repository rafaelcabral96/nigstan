<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Theoretical background | Fitting robust non-Gaussian models in Stan</title>
  <meta name="description" content="We illustrate in this Bookdown how to implement a generic class of non-Gaussian models in Stan." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Theoretical background | Fitting robust non-Gaussian models in Stan" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="We illustrate in this Bookdown how to implement a generic class of non-Gaussian models in Stan." />
  <meta name="github-repo" content="rafaelcabral96/nigstan" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Theoretical background | Fitting robust non-Gaussian models in Stan" />
  
  <meta name="twitter:description" content="We illustrate in this Bookdown how to implement a generic class of non-Gaussian models in Stan." />
  



<meta name="date" content="2022-10-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="implementing-latent-models-driven-by-nig-noise-in-stan.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/proj4-2.6.2/proj4.min.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.4.1/leaflet.js"></script>
<script src="libs/leaflet-providers-1.9.0/leaflet-providers_1.9.0.js"></script>
<script src="libs/leaflet-providers-plugin-2.0.4.1/leaflet-providers-plugin.js"></script>
<link href="libs/lfx-fullscreen-1.0.2/lfx-fullscreen-prod.css" rel="stylesheet" />
<script src="libs/lfx-fullscreen-1.0.2/lfx-fullscreen-prod.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Fitting robust non-Gaussian models</a></li>

<li class="divider"></li>
<li><a href="index.html#about" id="toc-about"><span class="toc-section-number">1</span> About</a>
<ul>
<li><a href="index.html#what-and-why" id="toc-what-and-why"><span class="toc-section-number">1.1</span> What and Why</a></li>
<li><a href="index.html#setup" id="toc-setup"><span class="toc-section-number">1.2</span> Setup</a></li>
<li><a href="index.html#this-bookdown" id="toc-this-bookdown"><span class="toc-section-number">1.3</span> This Bookdown</a></li>
<li><a href="index.html#citation" id="toc-citation"><span class="toc-section-number">1.4</span> Citation</a></li>
</ul></li>
<li><a href="theoretical-background.html#theoretical-background" id="toc-theoretical-background"><span class="toc-section-number">2</span> Theoretical background</a>
<ul>
<li><a href="theoretical-background.html#nig-distribution" id="toc-nig-distribution"><span class="toc-section-number">2.1</span> NIG distribution</a></li>
<li><a href="theoretical-background.html#new-parameterization" id="toc-new-parameterization"><span class="toc-section-number">2.2</span> New parameterization</a>
<ul>
<li><a href="theoretical-background.html#mean-scale-invariant-parameterization-eta-zeta" id="toc-mean-scale-invariant-parameterization-eta-zeta"><span class="toc-section-number">2.2.1</span> Mean-scale invariant parameterization (<span class="math inline">\(\eta\)</span>, <span class="math inline">\(\zeta\)</span>)</a></li>
<li><a href="theoretical-background.html#standardized-and-orthogonal-parameterization-etastar-zetastar" id="toc-standardized-and-orthogonal-parameterization-etastar-zetastar"><span class="toc-section-number">2.2.2</span> Standardized and orthogonal parameterization (<span class="math inline">\(\eta^\star\)</span>, <span class="math inline">\(\zeta^\star\)</span>)</a></li>
</ul></li>
<li><a href="theoretical-background.html#framework-for-extending-gaussian-models" id="toc-framework-for-extending-gaussian-models"><span class="toc-section-number">2.3</span> Framework for extending Gaussian models</a>
<ul>
<li><a href="theoretical-background.html#illustration-with-the-rw1-process" id="toc-illustration-with-the-rw1-process"><span class="toc-section-number">2.3.1</span> Illustration with the RW1 process</a></li>
<li><a href="theoretical-background.html#models-defined-via-mathbfdmathbfx-mathbfz" id="toc-models-defined-via-mathbfdmathbfx-mathbfz"><span class="toc-section-number">2.3.2</span> Models defined via <span class="math inline">\(\mathbf{D}\mathbf{x} = \mathbf{Z}\)</span></a></li>
<li><a href="theoretical-background.html#generic-framework" id="toc-generic-framework"><span class="toc-section-number">2.3.3</span> Generic framework</a></li>
<li><a href="theoretical-background.html#sample-paths" id="toc-sample-paths"><span class="toc-section-number">2.3.4</span> Sample paths</a></li>
</ul></li>
<li><a href="theoretical-background.html#penalized-complexity-priors-for-etastar-and-zetastar" id="toc-penalized-complexity-priors-for-etastar-and-zetastar"><span class="toc-section-number">2.4</span> Penalized complexity priors for <span class="math inline">\(\eta^\star\)</span> and <span class="math inline">\(\zeta^\star\)</span></a></li>
<li><a href="theoretical-background.html#useful-properties-of-the-vector-mathbfx" id="toc-useful-properties-of-the-vector-mathbfx"><span class="toc-section-number">2.5</span> Useful properties of the vector <span class="math inline">\(\mathbf{x}\)</span></a>
<ul>
<li><a href="theoretical-background.html#joint-pdf-of-mathbfx" id="toc-joint-pdf-of-mathbfx"><span class="toc-section-number">2.5.1</span> Joint PDF of <span class="math inline">\(\mathbf{x}\)</span></a></li>
<li><a href="theoretical-background.html#mixing-distribution-vector-mathbfv" id="toc-mixing-distribution-vector-mathbfv"><span class="toc-section-number">2.5.2</span> Mixing distribution vector <span class="math inline">\(\mathbf{V}\)</span></a></li>
</ul></li>
</ul></li>
<li><a href="implementing-latent-models-driven-by-nig-noise-in-stan.html#implementing-latent-models-driven-by-nig-noise-in-stan" id="toc-implementing-latent-models-driven-by-nig-noise-in-stan"><span class="toc-section-number">3</span> Implementing latent models driven by NIG noise in Stan</a>
<ul>
<li><a href="implementing-latent-models-driven-by-nig-noise-in-stan.html#framework" id="toc-framework"><span class="toc-section-number">3.1</span> Framework</a></li>
<li><a href="implementing-latent-models-driven-by-nig-noise-in-stan.html#implementation" id="toc-implementation"><span class="toc-section-number">3.2</span> Implementation</a></li>
<li><a href="implementing-latent-models-driven-by-nig-noise-in-stan.html#additional-functions" id="toc-additional-functions"><span class="toc-section-number">3.3</span> Additional functions</a>
<ul>
<li><a href="implementing-latent-models-driven-by-nig-noise-in-stan.html#nig-observations" id="toc-nig-observations"><span class="toc-section-number">3.3.1</span> NIG observations</a></li>
<li><a href="implementing-latent-models-driven-by-nig-noise-in-stan.html#sparse-matrix-computations" id="toc-sparse-matrix-computations"><span class="toc-section-number">3.3.2</span> Sparse matrix computations</a></li>
</ul></li>
<li><a href="implementing-latent-models-driven-by-nig-noise-in-stan.html#notes" id="toc-notes"><span class="toc-section-number">3.4</span> Notes</a>
<ul>
<li><a href="implementing-latent-models-driven-by-nig-noise-in-stan.html#non-centered-parameterization" id="toc-non-centered-parameterization"><span class="toc-section-number">3.4.1</span> Non-centered parameterization</a></li>
<li><a href="implementing-latent-models-driven-by-nig-noise-in-stan.html#heavy-tailed-distributions-and-stan" id="toc-heavy-tailed-distributions-and-stan"><span class="toc-section-number">3.4.2</span> Heavy-tailed distributions and Stan</a></li>
<li><a href="implementing-latent-models-driven-by-nig-noise-in-stan.html#determinant" id="toc-determinant"><span class="toc-section-number">3.4.3</span> Determinant</a></li>
</ul></li>
</ul></li>
<li><a href="simulations.html#simulations" id="toc-simulations"><span class="toc-section-number">4</span> Simulations</a>
<ul>
<li><a href="simulations.html#libraries-and-simulated-data" id="toc-libraries-and-simulated-data"><span class="toc-section-number">4.1</span> Libraries and simulated data</a></li>
<li><a href="simulations.html#fit-with-variance-mean-mixture-representation" id="toc-fit-with-variance-mean-mixture-representation"><span class="toc-section-number">4.2</span> Fit with Variance-mean mixture representation</a></li>
<li><a href="simulations.html#fit-with-nig_model" id="toc-fit-with-nig_model"><span class="toc-section-number">4.3</span> Fit with nig_model</a></li>
<li><a href="simulations.html#fit-with-nig_model_2" id="toc-fit-with-nig_model_2"><span class="toc-section-number">4.4</span> Fit with nig_model_2</a></li>
<li><a href="simulations.html#comparizon" id="toc-comparizon"><span class="toc-section-number">4.5</span> Comparizon</a></li>
</ul></li>
<li><a href="time-series.html#time-series" id="toc-time-series"><span class="toc-section-number">5</span> Time series</a>
<ul>
<li><a href="time-series.html#autogressive-process-driven-by-nig-noise" id="toc-autogressive-process-driven-by-nig-noise"><span class="toc-section-number">5.1</span> Autogressive process driven by NIG noise</a>
<ul>
<li><a href="time-series.html#libraries-and-data" id="toc-libraries-and-data"><span class="toc-section-number">5.1.1</span> Libraries and data</a></li>
<li><a href="time-series.html#priors" id="toc-priors"><span class="toc-section-number">5.1.2</span> Priors</a></li>
<li><a href="time-series.html#gaussian-fit" id="toc-gaussian-fit"><span class="toc-section-number">5.1.3</span> Gaussian fit</a></li>
<li><a href="time-series.html#nig-fit" id="toc-nig-fit"><span class="toc-section-number">5.1.4</span> NIG fit</a></li>
<li><a href="time-series.html#comparizon-1" id="toc-comparizon-1"><span class="toc-section-number">5.1.5</span> Comparizon</a></li>
<li><a href="time-series.html#predictions" id="toc-predictions"><span class="toc-section-number">5.1.6</span> Predictions</a></li>
</ul></li>
</ul></li>
<li><a href="spatial-matérn-model.html#spatial-matérn-model" id="toc-spatial-matérn-model"><span class="toc-section-number">6</span> Spatial Matérn model</a>
<ul>
<li><a href="spatial-matérn-model.html#introduction" id="toc-introduction"><span class="toc-section-number">6.1</span> Introduction</a></li>
<li><a href="spatial-matérn-model.html#implementation-1" id="toc-implementation-1"><span class="toc-section-number">6.2</span> Implementation</a></li>
<li><a href="spatial-matérn-model.html#libraries-and-data-1" id="toc-libraries-and-data-1"><span class="toc-section-number">6.3</span> Libraries and data</a></li>
<li><a href="spatial-matérn-model.html#discretization-mesh" id="toc-discretization-mesh"><span class="toc-section-number">6.4</span> Discretization mesh</a></li>
<li><a href="spatial-matérn-model.html#data-plot" id="toc-data-plot"><span class="toc-section-number">6.5</span> Data plot</a></li>
<li><a href="spatial-matérn-model.html#priors-1" id="toc-priors-1"><span class="toc-section-number">6.6</span> Priors</a></li>
<li><a href="spatial-matérn-model.html#stan-fit-with-a-gaussian-model" id="toc-stan-fit-with-a-gaussian-model"><span class="toc-section-number">6.7</span> Stan fit with a Gaussian model</a></li>
<li><a href="spatial-matérn-model.html#stan-fit-with-a-nig-driving-noise" id="toc-stan-fit-with-a-nig-driving-noise"><span class="toc-section-number">6.8</span> Stan fit with a NIG driving noise</a></li>
<li><a href="spatial-matérn-model.html#leave-one-out-cross-validation" id="toc-leave-one-out-cross-validation"><span class="toc-section-number">6.9</span> Leave-one-out cross validation</a></li>
<li><a href="spatial-matérn-model.html#prediction" id="toc-prediction"><span class="toc-section-number">6.10</span> Prediction</a>
<ul>
<li><a href="spatial-matérn-model.html#posterior-distribution-of-mathbfvoslashmathbfh" id="toc-posterior-distribution-of-mathbfvoslashmathbfh"><span class="toc-section-number">6.10.1</span> Posterior distribution of <span class="math inline">\(\mathbf{V}\oslash\mathbf{h}\)</span></a></li>
</ul></li>
</ul></li>
<li><a href="sar-and-car-models.html#sar-and-car-models" id="toc-sar-and-car-models"><span class="toc-section-number">7</span> SAR and CAR models</a>
<ul>
<li><a href="sar-and-car-models.html#sar-models" id="toc-sar-models"><span class="toc-section-number">7.1</span> SAR models</a>
<ul>
<li><a href="sar-and-car-models.html#libraries" id="toc-libraries"><span class="toc-section-number">7.1.1</span> Libraries</a></li>
</ul></li>
<li><a href="sar-and-car-models.html#columbus-dataset-and-model" id="toc-columbus-dataset-and-model"><span class="toc-section-number">7.2</span> Columbus dataset and model</a>
<ul>
<li><a href="sar-and-car-models.html#gaussian-fit-1" id="toc-gaussian-fit-1"><span class="toc-section-number">7.2.1</span> Gaussian fit</a></li>
<li><a href="sar-and-car-models.html#nig-fit-1" id="toc-nig-fit-1"><span class="toc-section-number">7.2.2</span> NIG fit</a></li>
</ul></li>
<li><a href="sar-and-car-models.html#car-models" id="toc-car-models"><span class="toc-section-number">7.3</span> CAR models</a>
<ul>
<li><a href="sar-and-car-models.html#dataset-and-model" id="toc-dataset-and-model"><span class="toc-section-number">7.3.1</span> Dataset and model</a></li>
<li><a href="sar-and-car-models.html#nig-fit-2" id="toc-nig-fit-2"><span class="toc-section-number">7.3.2</span> NIG fit</a></li>
<li><a href="sar-and-car-models.html#comparizon-2" id="toc-comparizon-2"><span class="toc-section-number">7.3.3</span> Comparizon</a></li>
<li><a href="sar-and-car-models.html#nig-model---relative-risk" id="toc-nig-model---relative-risk"><span class="toc-section-number">7.3.4</span> NIG model - Relative risk</a></li>
<li><a href="sar-and-car-models.html#gaussian-model---relative-risk" id="toc-gaussian-model---relative-risk"><span class="toc-section-number">7.3.5</span> Gaussian model - Relative Risk</a></li>
</ul></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fitting robust non-Gaussian models in Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="theoretical-background" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Theoretical background</h1>
<div id="nig-distribution" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> NIG distribution</h2>
<p>Instead of considering a Gaussian driving noise for the stochastic processes, we will use the NIG distribution, which is a more flexible distribution, allowing for exponential tails and asymmetry. The common parameterization of the NIG distribution has 4 parameters (<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\delta\)</span>, <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>) and the following pdf:</p>
<p><span class="math display">\[
\pi(x) = \frac{\alpha \delta K_{1}\left(\alpha \sqrt{\delta^{2}+(x-\mu)^{2}}\right)}{\pi \sqrt{\delta^{2}+(x-\mu)^{2}}} e^{\delta \gamma+\beta(x-\mu)},
\]</span></p>
<p>where <span class="math inline">\(\gamma= \sqrt{\alpha^2-\beta^2}\)</span>, and <span class="math inline">\(K_\nu(x)\)</span> is the modified Bessel function of the second kind. A NIG random variable <span class="math inline">\(\Lambda\)</span> can be represented as a Normal variance-mean mixture:
<span class="math display">\[
\Lambda|V \sim N(\mu+ \beta V , V) \\
V \sim IG(\delta,\gamma),
\]</span>
where IG stands for the Inverse-Gaussian distribution.</p>
<p>This representation is useful for sampling and is also the basis for Bayesian and maximum likelihood estimation (see <span class="citation">Bolin (<a href="#ref-bolin2014spatial" role="doc-biblioref">2014</a>)</span> and <span class="citation">Walder and Hanks (<a href="#ref-walder2020bayesian" role="doc-biblioref">2020</a>)</span>) since the normal variance-mean mixture shares many of the convenient properties of the Gaussian distribution.</p>
</div>
<div id="new-parameterization" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> New parameterization</h2>
<div id="mean-scale-invariant-parameterization-eta-zeta" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Mean-scale invariant parameterization (<span class="math inline">\(\eta\)</span>, <span class="math inline">\(\zeta\)</span>)</h3>
<p>In the previous parameterization, the location (<span class="math inline">\(\mu\)</span>) and scale (<span class="math inline">\(\delta\)</span>) parameters do not correspond to the mean and standard deviation of the distribution, which also depends on <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. In <span class="citation">Cabral, Bolin, and Rue (<a href="#ref-cabral2022controlling" role="doc-biblioref">2022</a>)</span> we propose a “mean-scale” invariant parameterization where the mean and the standard deviation are fixed. This not only eases interpretation but also allows assigning the priors for the mean and scale of the Gaussian model in non-Gaussian models, which is very convenient when implementing these non-Gaussian models in practice. The mixture representation belonging to this parameterization is:</p>
<p><span class="math display" id="eq:noise0">\[\begin{equation}
\Lambda|V \sim N\left(\zeta \tilde{\sigma}(V-1), \ \ \tilde{\sigma}^2V\right), \ \ \ \ \tilde{\sigma} = \frac{1}{\sqrt{1+\eta \zeta^2}}, \\
V \sim IG(1,\eta^{-1})
\tag{2.1}
\end{equation}\]</span></p>
<p>The mean and variance are always <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, respectively. The parameter <span class="math inline">\(\eta\)</span> is related to the degree of non-Gaussianity since <span class="math inline">\(\Lambda\)</span> converges to a Gaussian random variable when <span class="math inline">\(\eta \to 0\)</span>, and to a Cauchy random variable when <span class="math inline">\(\eta \to \infty\)</span>. The parameter <span class="math inline">\(\zeta\)</span> is related to the asymmetry of the random variable since <span class="math inline">\(\Lambda\)</span> is a symmetric random variable when <span class="math inline">\(\zeta=0\)</span>, and when <span class="math inline">\(\zeta&gt;0\)</span> it is skewed to the right. Also, the larger <span class="math inline">\(\eta\)</span>, the larger the asymmetry induced by the same value of <span class="math inline">\(\zeta\)</span> (see following plots).</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="theoretical-background.html#cb3-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&quot;files/images/NIGplot.png&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:NIGplot"></span>
<img src="files/images/NIGplot.png" alt="PDF of NIG distribution in log scale" width="100%" />
<p class="caption">
Figure 2.1: PDF of NIG distribution in log scale
</p>
</div>
</div>
<div id="standardized-and-orthogonal-parameterization-etastar-zetastar" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Standardized and orthogonal parameterization (<span class="math inline">\(\eta^\star\)</span>, <span class="math inline">\(\zeta^\star\)</span>)</h3>
<p>There is still some confounding between <span class="math inline">\(\eta\)</span> and <span class="math inline">\(\zeta\)</span> in the standardized parameterization since the excess kurtosis increases with <span class="math inline">\(\zeta\)</span>. We find the kurtosis to be hard to interpret (it is not clear what an increase in kurtosis of 1 means in practice) and so we prefer to associate <span class="math inline">\(\eta\)</span> with the likelihood of large events. For the NIG distribution this probability can be approximated by for large <span class="math inline">\(x\)</span> . The dependency of this probability with the skewness parameter <span class="math inline">\(\zeta\)</span> comes through the rate <span class="math inline">\(\xi\)</span>:
<span class="math display">\[\xi = 1+\zeta^2\eta - |\zeta|\sqrt{\eta(1+\zeta^2\eta)},\]</span>
which is equal to 1 in the symmetric case (<span class="math inline">\(\zeta=0\)</span>). <span class="citation">Cabral, Bolin, and Rue (<a href="#ref-cabral2022controlling" role="doc-biblioref">2022</a>)</span> required the probability <span class="math inline">\(P(|\Lambda|&gt;x)\)</span> to be invariant with the skewness parameter, at least for large <span class="math inline">\(x\)</span>. This can be achieved by the parameter transformations <span class="math inline">\(\eta^\star = \eta\xi_{NIG}^{-2}\)</span> and <span class="math inline">\(\zeta^\star = \zeta\sqrt{\eta}\)</span>. This parameterization has an orthogonal interpretation since the likelihood of large events and the excess Kurtosis now only depend on <span class="math inline">\(\eta^{\star}\)</span>.</p>
</div>
</div>
<div id="framework-for-extending-gaussian-models" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Framework for extending Gaussian models</h2>
<div id="illustration-with-the-rw1-process" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Illustration with the RW1 process</h3>
<p>Here we show how a random walk of order 1 (RW1) process with irregularly spaced locations can be extended to non-Gaussianity. It is quite a simple model, but it serves as a good illustration since the same procedure applies to more complex models. Consider a set of <span class="math inline">\(n\)</span> locations <span class="math inline">\(s_1,\dotsc, s_n\)</span>, where the distance between locations is <span class="math inline">\(h_i=s_{i+1}-s_i\)</span>. When the driving noise is Gaussian, we assume <span class="math inline">\(x_{i+1}-x_i \overset{d}{=} \sigma Z_i\)</span>, where <span class="math inline">\(Z_i \sim N(0,h_i)\)</span>. Notice that the RW1 model can be seen as an approximation of a Wiener process. In our non-Gaussian setting, we replace the standard Gaussian noise <span class="math inline">\(Z_i\)</span> with standardized non-Gaussian noise <span class="math inline">\(\Lambda_i\)</span>, so we assume that <span class="math inline">\(x_{i+1}-x_i \overset{d}{=} \sigma\Lambda_i\)</span>, where <span class="math inline">\(\Lambda_i\)</span> follows:</p>
<p><span class="math display" id="eq:noise">\[\begin{equation}
\Lambda_i|V_i \sim N\left(\zeta \tilde{\sigma}(V_i-h_i), \ \ \tilde{\sigma}^2V_i\right), \ \ \ \ \tilde{\sigma} = \frac{\sigma}{\sqrt{1+\eta \zeta^2}} \\
V_{i} \overset{ind.}{\sim} IG(h_i,\eta^{-1} h_i^2)   \\
\tag{2.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(E[\Lambda_i]=0\)</span> and <span class="math inline">\(Var[\Lambda_i]=h_i\)</span>. For the analytically inclined, this RW1 model is an approximation of a Lévy process <span class="math inline">\(L(t)\)</span> where an increment of size 1 follows the NIG distribution. Speaking only of essential terms, a Lévy process is a process with independent and stationary increments, and all finite-dimensional distributions are known if we know the distribution of an increment (more about Lévy processes in <span class="citation">Ken-Iti (<a href="#ref-ken1999levy" role="doc-biblioref">1999</a>)</span> and <span class="citation">Barndorff-Nielsen, Mikosch, and Resnick (<a href="#ref-barndorff2012levy" role="doc-biblioref">2012</a>)</span>). If the increments are Gaussian then <span class="math inline">\(L(t)\)</span> is a Gaussian process, while in general, it is non-Gaussian. <!--The parameterization ($\eta^\star$,$\zeta^\star$) also depends now on a predefined vector $\mathbf{h}$, which contains the distance between locations.--></p>
<p>Next, we show a sample path of RW1 with <span class="math inline">\(\sigma=1\)</span> near the Gaussian case by setting <span class="math inline">\(\eta=10^{-6}\)</span> (top) and a non-Gaussian case by setting <span class="math inline">\(\eta=10\)</span> (bottom). We generated these simulated paths by first sampling the mixing variables <span class="math inline">\(V_i\)</span>, then sampling the noise <span class="math inline">\(\Lambda_i|V_i\)</span>, and then the RW1 sample paths are just a cumulative sum of the noises <span class="math inline">\(\Lambda_i\)</span>. Notice that, in the first panel, the mixing variables <span class="math inline">\(V_i\)</span> are very close to 1 while on the bottom panel there is more discrepancy. When <span class="math inline">\(V_i\)</span> takes a large value, the noise <span class="math inline">\(\Lambda\)</span> can also be large and then we observe a large jump in the RW1 process driven by NIG noise.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="theoretical-background.html#cb4-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&quot;files/images/GaussRW1.png&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="files/images/GaussRW1.png" alt="Sample of mixing vector $\mathbf{V}$, noise vector $\mathbf{\Lambda}$ and RW1 vector $\mathbf{x}$." width="100%" />
<p class="caption">
Figure 2.2: Sample of mixing vector <span class="math inline">\(\mathbf{V}\)</span>, noise vector <span class="math inline">\(\mathbf{\Lambda}\)</span> and RW1 vector <span class="math inline">\(\mathbf{x}\)</span>.
</p>
</div>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="theoretical-background.html#cb5-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&quot;files/images/NIGRW1.png&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="files/images/NIGRW1.png" alt="Sample of mixing vector $\mathbf{V}$, noise vector $\mathbf{\Lambda}$ and RW1 vector $\mathbf{x}$." width="100%" />
<p class="caption">
Figure 2.3: Sample of mixing vector <span class="math inline">\(\mathbf{V}\)</span>, noise vector <span class="math inline">\(\mathbf{\Lambda}\)</span> and RW1 vector <span class="math inline">\(\mathbf{x}\)</span>.
</p>
</div>
</div>
<div id="models-defined-via-mathbfdmathbfx-mathbfz" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Models defined via <span class="math inline">\(\mathbf{D}\mathbf{x} = \mathbf{Z}\)</span></h3>
<p>The system of equations seen before <span class="math inline">\(x_{i+1}-x_i \overset{d}{=} \sigma Z_i\)</span> defines the linear system <span class="math inline">\(\mathbf{D}_{RW1}\mathbf{x}=\sigma\mathbf{Z}\)</span>, where <span class="math inline">\(\mathbf{Z}\)</span> is a vector of independent Gaussian noise, and:
<span class="math display">\[
\mathbf{D}_{RW1} = \begin{pmatrix}
-1 &amp; 1  &amp;  &amp; &amp; \\
&amp; -1 &amp; 1 &amp; &amp; \\
&amp; &amp; \ddots  &amp; \ddots &amp;              \\
&amp; &amp; &amp; -1 &amp; 1
\end{pmatrix}.
\]</span></p>
<p>For a RW2 model it is assumed <span class="math inline">\(x_{i+2}-2x_{i+1}+x_i = \sigma Z_i\)</span>, and for an autoregressive process of order 1 (AR1) the assumptions are that <span class="math inline">\(\sqrt{1-\rho^2}x_1= \sigma Z_1\)</span>, and <span class="math inline">\(x_{i+1}-\rho x_i = \sigma Z_i\)</span> for <span class="math inline">\(i&gt;1\)</span>, where <span class="math inline">\(\rho\)</span> is the autocorrelation parameter. Thus, the matrices <span class="math inline">\(\mathbf{D}_{RW2}\)</span> and <span class="math inline">\(\mathbf{D}_{AR1}\)</span> are:</p>
<p><span class="math display">\[
\mathbf{D}_{RW2} = \begin{pmatrix}
1 &amp; -2  &amp; 1 &amp; &amp; &amp; \\
&amp; 1 &amp; -2 &amp; 1 &amp; &amp; \\
\\
&amp; &amp; &amp; \ddots  &amp; \ddots &amp; \ddots   &amp;           \\
\\
&amp; &amp; &amp;  &amp; &amp; 1 &amp; -2 &amp; 1
\end{pmatrix}, \ \
\mathbf{D}_{AR1} = \begin{pmatrix}
\sqrt{1-\rho^2} &amp;   &amp;  &amp; &amp; &amp; \\
-\rho &amp; 1 &amp;  &amp;  &amp; &amp; \\
  &amp; -\rho &amp; 1 &amp;  &amp; &amp; \\
&amp;  &amp; \ddots &amp; \ddots   &amp; \\
&amp;  &amp;  &amp;  -\rho &amp; 1
\end{pmatrix}
\]</span>
Likewise, higher-order AR processes can be constructed. The rows of the matrix <span class="math inline">\(\mathbf{D}\)</span> contain the “increments” that we assume to follow Gaussian noise. Other models fit in this framework, including intrinsic conditional autoregressive models and simultaneous autoregressive models which we will see later in the applications. The system <span class="math inline">\(\mathbf{D}\mathbf{x} = \sigma\mathbf{Z}\)</span> also appears when computing discrete-space approximation of stochastic processes defined via SPDEs: <span class="math inline">\(\mathcal{D}X(t) = W&#39;(t)\)</span>, where <span class="math inline">\(\mathcal{D}\)</span> is a linear differential operator and <span class="math inline">\(W&#39;(t)\)</span> is a Gaussian noise process. For instance, stationary Matérn random fields and Ornstein–Uhlenbeck processes arise as stationary solutions to SPDEs.</p>
</div>
<div id="generic-framework" class="section level3" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Generic framework</h3>
<p>We have seen before that the linear system
<span class="math display">\[
\mathbf{D}\mathbf{x}^G\overset{d}{=} \mathbf{Z},
\]</span>
is a natural representation for many models, where the matrix <span class="math inline">\(\mathbf{D}\)</span> defines the model and <span class="math inline">\(\mathbf{Z}\)</span> is a vector of independent Gaussian noise: <span class="math inline">\(Z_i \overset{i.i.d}{\sim} N(0,h_i)\)</span>, where <span class="math inline">\(h_i\)</span> is the distance between locations, for instance. The precision matrix of <span class="math inline">\(\mathbf{x}\)</span> is then <span class="math inline">\(\mathbf{Q}\propto \mathbf{D}^T\text{diag}(\mathbf{h})^{-1}\mathbf{D}\)</span>, which for the examples we will study is sparse. The non-Gaussian extension consists in assuming:</p>
<p><span class="math display">\[
\mathbf{D}\mathbf{x}\overset{d}{=} \mathbf{\Lambda},
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Lambda}\)</span> is a vector of independent NIG noise as in eq. <a href="theoretical-background.html#eq:noise">(2.2)</a>. A location and scale parameter can be added by the usual transformation <span class="math inline">\(\mu + \sigma\mathbf{x}\)</span>. Consider the vector <span class="math inline">\(\mathbf{V}=[V_1,V_2, \dotsc, V_n]^T\)</span>. If <span class="math inline">\(\mathbf{D}\)</span> is a full rank matrix, then <span class="math inline">\(\mathbf{x}\)</span> has the representation:</p>
<p><span class="math display" id="eq:framework">\[\begin{equation}
\mathbf{x}|\boldsymbol{V} \sim N\left(\tilde{\sigma}\zeta \mathbf{D}^{-1}(\boldsymbol{V}-\boldsymbol{h}), \ \ \tilde{\sigma}^2 \mathbf{D}^{-1}\text{diag}(\boldsymbol{V})\mathbf{D}^{-T}\right)\\
V_{i} \overset{ind.}{\sim}  IG(h_i,\eta^{-1} h_i^2)   \\
\tag{2.3}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}|\boldsymbol{V}\)</span> is still a multivariate Gaussian distribution, but with precision matrix <span class="math inline">\(Q=\tilde{\sigma}^{-2}\mathbf{D}^T \text{diag} ({\mathbf{V}})^{-1}\mathbf{D}\)</span> with the same degree of sparsity as in the Gaussian case. Here it is important to mention that the precision or covariance matrices do not uniquely define the models, and these are only uniquely defined by the matrix <span class="math inline">\(\mathbf{D}\)</span>. For instance, if we decompose the covariance matrix in two ways, <span class="math inline">\(\Sigma = L_1 L_1^T\)</span> and <span class="math inline">\(\Sigma = L_2 L_2^T\)</span>, then in the Gaussian case, the vectors <span class="math inline">\(\mathbf{x_1}=\mathbf{L_1}\mathbf{Z}\)</span> and <span class="math inline">\(\mathbf{x_2}=\mathbf{L_2}\mathbf{Z}\)</span> are equal in distribution. However, when the noise is non-Gaussian this is not the case. An example is shown in Fig. 2 of <span class="citation">Asar et al. (<a href="#ref-DavidLinear" role="doc-biblioref">2020</a>)</span>, where stochastic processes with associated operators <span class="math inline">\(\mathcal{D}_1=\kappa + \partial_t\)</span>, <span class="math inline">\(\mathcal{D}_2 =\kappa^2- \partial_t^2\)</span> have the same exponential covariance function, but when using a NIG noise process the sample paths behave differently. This limits the transformations we can do for computational efficiency because we are restricted to the model representation <span class="math inline">\(\mathbf{D}\mathbf{x}=\mathbf{\Lambda}\)</span>.</p>
</div>
<div id="sample-paths" class="section level3" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Sample paths</h3>
<p>We look at different models that can be expressed through:
<span class="math display">\[
\mathcal{D}X(t) =L&#39;(t),
\]</span>
where <span class="math inline">\(\mathcal{D}\)</span> is a linear differential operator and <span class="math inline">\(L&#39;(t)\)</span> is a NIG noise process. The differential operator for different models is shown next, where OU stands for the Ornstein–Uhlenbeck process, which can be seen as the continuous version of an autoregressive process of order 1. In the third column, we consider the discrete case equivalents of these models, where the ‘increments’ are assumed to follow independent NIG noise.</p>
<table>
<thead>
<tr class="header">
<th align="left">Model</th>
<th><span class="math inline">\(\mathcal{D}\)</span></th>
<th>Increments</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">RW1</td>
<td><span class="math inline">\(\partial_t\)</span></td>
<td><span class="math inline">\(x_{i+1}-x_{i}\)</span></td>
</tr>
<tr class="even">
<td align="left">OU</td>
<td><span class="math inline">\(\kappa + \partial_t\)</span></td>
<td><span class="math inline">\(x_{i+1} + (\kappa-1)x_{i}\)</span></td>
</tr>
<tr class="odd">
<td align="left">RW2</td>
<td><span class="math inline">\(\partial_t^2\)</span></td>
<td><span class="math inline">\(x_{i} - 2x_{i+1} + x_{i+2}\)</span></td>
</tr>
<tr class="even">
<td align="left">Matérn <span class="math inline">\(\alpha=2\)</span></td>
<td><span class="math inline">\(\kappa^2- \partial_t^2\)</span></td>
<td><span class="math inline">\((1+\kappa^2)x_{i} - 2x_{i+1} + x_{i+2}\)</span></td>
</tr>
</tbody>
</table>
<p>The first column of the next plot shows a simulation of Gaussian noise and the sample paths of several models generated from it. In the second column, we repeat the same, but now with non-Gaussian NIG noise. We can see that most NIG noise events are near 0, and a few of them take large values (larger than the ones you see in the Gaussian noise process) since the NIG distribution has heavier tails and is more peaked around 0. Whenever the noise takes a large value (for instance, near location 0.25), the RW1 and OU processes will exhibit a distinct jump, and the RW2 and Matérn processes will exhibit a kink (discontinuity in the first derivative).</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="theoretical-background.html#cb6-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&quot;files/images/sim1.png&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="files/images/sim1.png" alt="Noise and sample paths for several models" width="100%" />
<p class="caption">
Figure 2.4: Noise and sample paths for several models
</p>
</div>
<p>Next, we show sample paths of a Matérn model in 2D driven with NIG driving noise, as described in <span class="citation">Bolin (<a href="#ref-bolin2014spatial" role="doc-biblioref">2014</a>)</span>. Notice that in Fig. <a href="theoretical-background.html#fig:compareSPDE">2.5</a> as <span class="math inline">\(\eta\)</span> increases we observe more “hotspots” and “coldspots”, that is, regions where the field <span class="math inline">\(X\)</span> takes values that would be considered extreme in a Gaussian model. In the symmetric case (<span class="math inline">\(\zeta = 0\)</span>) the number of hotspots and coldspots is on average the same, while if <span class="math inline">\(\zeta &gt; 0\)</span> there should be on average more hotspots than coldspots.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="theoretical-background.html#cb7-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&quot;files/images/sim3.png&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:compareSPDE"></span>
<img src="files/images/sim3.png" alt="Sample paths of a Matérn model in 2D with $\alpha=2$" width="100%" />
<p class="caption">
Figure 2.5: Sample paths of a Matérn model in 2D with <span class="math inline">\(\alpha=2\)</span>
</p>
</div>
</div>
</div>
<div id="penalized-complexity-priors-for-etastar-and-zetastar" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Penalized complexity priors for <span class="math inline">\(\eta^\star\)</span> and <span class="math inline">\(\zeta^\star\)</span></h2>
<p>The non-Gaussian extension does not affect the mean and covariance structure of the model so we can reuse the same priors for the scale <span class="math inline">\(\sigma\)</span> and spatial range parameter <span class="math inline">\(\kappa\)</span> from the Gaussian model. Prior distributions for <span class="math inline">\(\eta^\star\)</span> and <span class="math inline">\(\zeta^\star\)</span> are constructed based on the penalized complexity (PC) priors approach (<span class="citation">Simpson et al. (<a href="#ref-simpson2017penalising" role="doc-biblioref">2017</a>)</span>) in <span class="citation">Cabral, Bolin, and Rue (<a href="#ref-cabral2022controlling" role="doc-biblioref">2022</a>)</span>. PC priors tend to avoid overfitting by default because the mode of the prior distribution is located at the base model.</p>
<p>A useful result shown in <span class="citation">Cabral, Bolin, and Rue (<a href="#ref-cabral2022controlling" role="doc-biblioref">2022</a>)</span> is that the PC priors for <span class="math inline">\(\eta^\star\)</span> and <span class="math inline">\(\zeta^\star\)</span> do not depend on the matrix <span class="math inline">\(\mathbf{D}\)</span> or the scale <span class="math inline">\(\sigma\)</span>, so the prior distributions will be the same regardless of the model. The PC prior for <span class="math inline">\(\eta^\star\)</span> follows an exponential distribution and the PC prior for <span class="math inline">\(\zeta^\star\)</span> follow a Laplace distribution, each with a given rate parameter. This rate parameter can be found by relating <span class="math inline">\(\eta^\star\)</span> and <span class="math inline">\(\zeta^\star\)</span> with some interpretable properties of the model. For <span class="math inline">\(\eta^\star\)</span> we can study how many more large events the NIG noise has compared with the Gaussian noise: <span class="math inline">\(Q(\eta^\star) = P(|\Lambda|&gt;3)/P(|Z|&gt;3)\)</span>, where <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(Z\)</span> are standardized NIG and Gaussian random variable. Then, <span class="math inline">\(\eta^\star\)</span> is found from the probability statement <span class="math inline">\(P(Q(\eta^\star)&gt;2)=\alpha\)</span>, where we set a low probability <span class="math inline">\(\alpha\)</span> that the NIG noise has twice as much large events as the Gaussian noise. A more detailed discussion on the PC priors for non-Gaussian models can be found in <span class="citation">Cabral, Bolin, and Rue (<a href="#ref-cabral2022controlling" role="doc-biblioref">2022</a>)</span></p>
</div>
<div id="useful-properties-of-the-vector-mathbfx" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Useful properties of the vector <span class="math inline">\(\mathbf{x}\)</span></h2>
<p>In some examples we consider latent (non-Gaussian) models where the observations <span class="math inline">\(\mathbf{y}\)</span> depend on <span class="math inline">\(\mathbf{x}\)</span> through <span class="math inline">\(\mathbf{y}|\mathbf{x} \sim N(\mathbf{B}\boldsymbol{\beta} + \sigma_x\mathbf{x},\sigma_y)\)</span>, where <span class="math inline">\(\mathbf{B}\)</span> is a design matrix, <span class="math inline">\(\boldsymbol{\beta}\)</span> are regression coefficients, and <span class="math inline">\(\mathbf{x}\)</span> is the non-Gaussian random vector usually added to model structured temporal or spatial effects.</p>
<p>Bayesian and maximum likelihood estimation of these models typically leverages the hierarchical representation in eq. <a href="theoretical-background.html#eq:framework">(2.3)</a>. The precision matrix of <span class="math inline">\(\mathbf{x}|\mathbf{V}\)</span> is sparse, but to compute the mean of <span class="math inline">\(\mathbf{x}|\mathbf{V}\)</span> we need to invert the matrix <span class="math inline">\(\mathbf{D}\)</span>, which can be expensive, especially if <span class="math inline">\(\mathbf{D}\)</span> depends on a parameter, since then this inversion needs to be done at every iteration. Furthermore, if we implement this representation directly on Stan, besides estimating, <span class="math inline">\(\sigma_x\)</span>, <span class="math inline">\(\eta^\star\)</span>, and <span class="math inline">\(\zeta^\star\)</span>, we need an extra <span class="math inline">\(2N\)</span> parameters, since both <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> need to be estimated, where <span class="math inline">\(N\)</span> is the dimension of <span class="math inline">\(\mathbf{x}\)</span>,</p>
<div id="joint-pdf-of-mathbfx" class="section level3" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Joint PDF of <span class="math inline">\(\mathbf{x}\)</span></h3>
<p>By integrating out the auxiliary variables <span class="math inline">\(V_i\)</span> in eq. <a href="theoretical-background.html#eq:framework">(2.3)</a> we can basically cut the dimension of the parameter space being explored in Stan by almost half, which should lead to a significant speedup. This at first may seem complicated to do, but if we realize that <span class="math inline">\(\mathbf{x} = \mathbf{D}^{-1}\mathbf{\Lambda}\)</span> if <span class="math inline">\(\mathbf{D}\)</span> is non-singular, then the multivariate transformation method wields:
<span class="math display" id="eq:joint">\[\begin{equation}
    \pi(\mathbf{x})= |\mathbf{D}|\prod_{i=1}^n\pi_{\Lambda_i}([\mathbf{D}\mathbf{x}]_i),
    \tag{2.4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\pi_{\Lambda_i}\)</span> is the PDF of a NIG distribution with parameters <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\eta^\star\)</span> and <span class="math inline">\(\zeta^\star\)</span>, and <span class="math inline">\(h_i\)</span> as in eq. <a href="theoretical-background.html#eq:noise">(2.2)</a>. We can then do the estimation in Stan based on the previous joint density, where there is no need for the auxiliary vector <span class="math inline">\(\mathbf{V}\)</span> and no need of inverting the matrix <span class="math inline">\(\mathbf{D}\)</span>.</p>
</div>
<div id="mixing-distribution-vector-mathbfv" class="section level3" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Mixing distribution vector <span class="math inline">\(\mathbf{V}\)</span></h3>
<p>The prior assumption on the mixing variables is <span class="math inline">\(V_{i}|\eta \overset{ind.}{\sim} IG(h_i,\eta^{-1} h_i^2)\)</span>. It can be shown that <span class="math inline">\(V_{i} \to h_i\)</span>, in the Gaussian limit (when <span class="math inline">\(\eta \to 0\)</span>). Therefore in the Gaussian limit <span class="math inline">\(\mathbf{V}\oslash\mathbf{h}=\mathbf{1}\)</span>, where <span class="math inline">\(\oslash\)</span> performs the element-wise division. If <span class="math inline">\(V_i/h_i\)</span> has a high posterior mean, then the noise element <span class="math inline">\(\Lambda_i\)</span> is significantly non-Gaussian, and more flexibility is needed around time point or location <span class="math inline">\(i\)</span> that is not present in the Gaussian model (see Fig. <a href="#fig:RW1sample"><strong>??</strong></a>). Thus, the posterior summaries of <span class="math inline">\(E[\mathbf{V}\oslash\mathbf{h}]\)</span> can be used as a diagnostic tool to inspect if and where departures from Gaussianity occur. It is easier to interpret than the posterior summaries of <span class="math inline">\(\eta\)</span>, as the “non-Gaussianity” of <span class="math inline">\(\mathbf{x}\)</span> depends not only on <span class="math inline">\(\eta\)</span>, but also on <span class="math inline">\(\mathbf{D}\)</span> (<span class="citation">Cabral, Bolin, and Rue (<a href="#ref-cabral2022controlling" role="doc-biblioref">2022</a>)</span>).</p>
<p>After the estimation is completed using the joint PDF of eq. <a href="theoretical-background.html#eq:joint">(2.4)</a> we can generate samples of <span class="math inline">\(\mathbf{V}|\mathbf{y}\)</span> post-hoc using:</p>
<p><span class="math display" id="eq:V">\[\begin{equation}
V_i|\mathbf{x},\tilde{\sigma},\eta,\zeta,h_i \sim GIG(-1,\eta^{-1} + \zeta^2,\eta^{-1}h_i^2 + ([\mathbf{D}\mathbf{x}]_i/\tilde{\sigma} + \zeta h_i)^2),
\tag{2.5}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(GIG(p, a,b)\)</span> stands for the generalized inverse Gaussian distribution with pdf:</p>
<p><span class="math display">\[
\pi(x | p, a, b)=\frac{(a / b)^{p / 2}}{2 K_{p}(\sqrt{a b})} x^{p-1} \exp \left(-\frac{a}{2} x-\frac{b}{2} x^{-1}\right),\ \ x&gt;0.
\]</span></p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-DavidLinear" class="csl-entry">
Asar, Özgür, David Bolin, Peter J. Diggle, and Jonas Wallin. 2020. <span>“Linear Mixed Effects Models for Non-Gaussian Continuous Repeated Measurement Data.”</span> <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em> 69 (5): 1015–65. <a href="https://doi.org/doi.org/10.1111/rssc.12405">https://doi.org/doi.org/10.1111/rssc.12405</a>.
</div>
<div id="ref-barndorff2012levy" class="csl-entry">
Barndorff-Nielsen, Ole E, Thomas Mikosch, and Sidney I Resnick. 2012. <em>L<span>é</span>vy Processes: Theory and Applications</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-bolin2014spatial" class="csl-entry">
Bolin, David. 2014. <span>“Spatial Mat<span>é</span>rn Fields Driven by Non-Gaussian Noise.”</span> <em>Scandinavian Journal of Statistics</em> 41 (3): 557–79.
</div>
<div id="ref-cabral2022controlling" class="csl-entry">
Cabral, Rafael, David Bolin, and Håvard Rue. 2022. <span>“Controlling the Flexibility of Non-Gaussian Processes Through Shrinkage Priors.”</span> <em>arXiv Preprint arXiv:2203.05510</em>.
</div>
<div id="ref-ken1999levy" class="csl-entry">
Ken-Iti, Sato. 1999. <em>L<span>é</span>vy Processes and Infinitely Divisible Distributions</em>. Cambridge university press.
</div>
<div id="ref-simpson2017penalising" class="csl-entry">
Simpson, Daniel, Håvard Rue, Andrea Riebler, Thiago G Martins, and Sigrunn H Sørbye. 2017. <span>“Penalising Model Component Complexity: A Principled, Practical Approach to Constructing Priors.”</span> <em>Statistical Science</em>, 1–28.
</div>
<div id="ref-walder2020bayesian" class="csl-entry">
Walder, Adam, and Ephraim M Hanks. 2020. <span>“Bayesian Analysis of Spatial Generalized Linear Mixed Models with Laplace Moving Average Random Fields.”</span> <em>Computational Statistics &amp; Data Analysis</em> 144: 106861.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="implementing-latent-models-driven-by-nig-noise-in-stan.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/01-theory.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
